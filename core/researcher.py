import asyncio
import os
from datetime import datetime
import random
import re
from pathlib import Path
import json
import traceback
import aiohttp
import io

import httpx
try:
    from playwright.async_api import Page, TimeoutError as PlaywrightTimeoutError
except ModuleNotFoundError:  # å…è®¸â€œä»…ä» JSON ç”ŸæˆæŠ¥å‘Šâ€åœºæ™¯ä¸å®‰è£… playwright
    Page = object  # type: ignore

    class PlaywrightTimeoutError(Exception):
        pass

from config.settings import (
    DEEP_RESEARCH_ENABLED,
    DEEP_RESEARCH_POST_LIMIT,
    DEEP_RESEARCH_LLM_MODEL,
    DEEP_RESEARCH_COMMENT_LIMIT,
    DEEP_RESEARCH_OUTPUT_DIR,
    SEARCH_KEYWORDS,
    BASE_URL,
    SELECTORS,
    ASR_SERVER_URL
)
from core.browser_manager import BrowserManager
from core.llm_client import LLMClient
from core.human_motion import HumanMotion
from core.video_downloader import VideoDownloader
from core.report_renderer import render_deep_research_html
from rapidocr import RapidOCR

class ResearchAgent:
    def __init__(self, browser_manager: BrowserManager, llm_client: LLMClient, recorder):
        self.browser_manager = browser_manager
        self.llm_client = llm_client
        self.page = browser_manager.page
        self.recorder = recorder  # ç»Ÿä¸€ä½¿ç”¨ recorder æ—¥å¿—ç³»ç»Ÿ
        self.human = HumanMotion(self.page)

        if not DEEP_RESEARCH_ENABLED:
            self.recorder.log("info", "æ·±åº¦ç ”ç©¶æ¨¡å¼æœªå¯ç”¨")
            return

        self.output_dir = DEEP_RESEARCH_OUTPUT_DIR / datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.recorder.log("info", f"ğŸ“‚ [æ·±åº¦ç ”ç©¶] è¾“å‡ºç›®å½•: {self.output_dir}")

        self.video_downloader = VideoDownloader(save_dir=self.output_dir / "videos")
        self.visited_note_ids = set()  # æ–°å¢ï¼šå·²è®¿é—®å¸–å­IDé›†åˆ
        self.ocr_engine = None
        if DEEP_RESEARCH_ENABLED:
            self.ocr_engine = RapidOCR()
            self.recorder.log("info", "ğŸ§  OCR å¼•æ“å·²åŠ è½½")

    async def run_deep_research(self, keyword: str = None):
        if not DEEP_RESEARCH_ENABLED:
            self.recorder.log("info", "Deep research mode is disabled. Skipping run.")
            return

        self.recorder.log("info", f"ğŸ“š [æ·±åº¦ç ”ç©¶] å¼€å§‹æ·±åº¦ç ”ç©¶: {keyword if keyword else 'configured keywords'}")

        search_term = keyword if keyword else random.choice(SEARCH_KEYWORDS)

        # æ‰§è¡Œæœç´¢
        await self._perform_search(search_term)

        # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æµè§ˆè¡Œä¸ºï¼šé€ä¸ªç‚¹å‡»å¸–å­
        research_data = []
        posts_processed = 0
        attempts = 0  # å°è¯•æ¬¡æ•°è®¡æ•°å™¨

        while posts_processed < DEEP_RESEARCH_POST_LIMIT:
            # 1. æ£€æŸ¥ç¯å¢ƒ
            if "xiaohongshu.com" not in self.page.url or "search_result" not in self.page.url:
                self.recorder.log("error", f"âŒ [æ·±åº¦ç ”ç©¶] ç¯å¢ƒåç¦»: {self.page.url}")
                break

            # 2. å¯»æ‰¾è§†å£å†…çš„å¸–å­
            notes = await self.page.locator(SELECTORS["note_card"]).all()
            if not notes:
                self.recorder.log("warning", "ğŸ“ [æ·±åº¦ç ”ç©¶] è§†å£æ— å¸–å­ï¼Œæ»šåŠ¨å¯»æ‰¾...")
                await self.human.human_scroll(500)
                await asyncio.sleep(2)
                notes = await self.page.locator(SELECTORS["note_card"]).all()
                if not notes:
                    self.recorder.log("error", "âŒ [æ·±åº¦ç ”ç©¶] æœªæ£€æµ‹åˆ°ç¬”è®°ï¼Œç»“æŸç ”ç©¶")
                    break

            # 3. é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰é®ç½©å±‚å­˜åœ¨ï¼ˆé¿å…ä¸Šæ¬¡å…³é—­å¤±è´¥ï¼‰
            try:
                mask_visible = await self.page.locator(SELECTORS["note_detail_mask"]).is_visible()
                if mask_visible:
                    self.recorder.log("warning", "âš ï¸ æ£€æµ‹åˆ°æ®‹ç•™é®ç½©å±‚ï¼Œå¼ºåˆ¶å…³é—­...")
                    await self.page.keyboard.press("Escape")
                    await self.page.wait_for_selector(
                        SELECTORS["note_detail_mask"],
                        state="hidden",
                        timeout=3000
                    )
                    await asyncio.sleep(0.5)
            except Exception as e:
                self.recorder.log("debug", f"é®ç½©å±‚æ£€æŸ¥: {e}")

            # 4. é€‰æ‹©ä¸€ä¸ªå¸–å­å¹¶ç‚¹å‡»ï¼ˆç ”ç©¶æ¨¡å¼ï¼šåŠ é€Ÿæµè§ˆï¼‰
            target_note = random.choice(notes[:6])  # ä»å‰6ä¸ªä¸­éšæœºé€‰æ‹©
            await target_note.scroll_into_view_if_needed()
            await asyncio.sleep(random.uniform(0.3, 0.5))  # å‡åŠå»¶è¿Ÿ

            # æå‰è·å– note_id ç”¨äºæ—¥å¿—ï¼ˆä»å­å…ƒç´  <a> æ ‡ç­¾è·å– hrefï¼‰
            note_id_preview = "unknown"
            try:
                # æ­£ç¡®è·å–ï¼šå…ˆå®šä½æ‰€æœ‰ <a> æ ‡ç­¾ï¼Œç„¶åå–ç¬¬ä¸€ä¸ªçš„ href
                note_links = target_note.locator('a[href*="/explore/"]')
                if await note_links.count() > 0:
                    note_href = await note_links.first.get_attribute('href') or ""
                    if note_href:
                        note_id_preview = self._extract_note_id_from_url(note_href)[:8] or "unknown"
            except Exception as e:
                self.recorder.log("debug", f"è·å– note_id å¤±è´¥: {e}")

            attempts += 1
            self.recorder.log("info", f"ğŸ‘† [æ·±åº¦ç ”ç©¶] ç‚¹å‡»ç¬¬ {attempts} ä¸ªå¸–å­ | å·²æ”¶é›†: {posts_processed}/{DEEP_RESEARCH_POST_LIMIT} (ID: {note_id_preview}...)")
            await target_note.click()

            # 5. ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½ï¼Œå¹¶å°è¯•ä»URLè·å–ID
            try:
                await self.page.wait_for_selector(SELECTORS["note_detail_mask"], timeout=5000)
                # å¦‚æœä¹‹å‰æ²¡è·å–åˆ°IDï¼Œå°è¯•ä»å½“å‰é¡µé¢URLè·å–
                if note_id_preview == "unknown":
                    current_url = self.page.url
                    note_id_from_url = self._extract_note_id_from_url(current_url)
                    if note_id_from_url:
                        note_id_preview = note_id_from_url[:8]
            except:
                self.recorder.log("warning", "â±ï¸ [æ·±åº¦ç ”ç©¶] è¯¦æƒ…é¡µåŠ è½½è¶…æ—¶ï¼Œè·³è¿‡æ­¤å¸–")
                await self.page.keyboard.press("Escape")
                continue

            # 6. æå–å¸–å­å†…å®¹ï¼ˆä¸è°ƒç”¨ LLMï¼Œä»…æå–æ•°æ®ï¼‰
            post_data = await self._extract_content_from_page()

            # åˆ¤æ–­å¸–å­æ˜¯å¦æœ‰ä»·å€¼ï¼šæ–‡å­—ã€å›¾ç‰‡ã€è§†é¢‘ã€è¯„è®ºä»»ä¸€å­˜åœ¨å³å¯æ”¶é›†
            # çº¯å›¾ç‰‡å¸–å­ã€æœ‰è¯„è®ºçš„å¸–å­éƒ½æ˜¯æœ‰ä»·å€¼çš„å†…å®¹ï¼
            has_value = False
            if post_data:
                has_value = bool(
                    post_data.get("content") or          # æœ‰æ–‡å­—å†…å®¹
                    post_data.get("image_urls") or       # æœ‰å›¾ç‰‡
                    post_data.get("video_url") or        # æœ‰è§†é¢‘
                    post_data.get("comments")            # æœ‰è¯„è®º
                )

            if has_value:
                research_data.append(post_data)
                posts_processed += 1
                note_id = self._extract_note_id_from_url(post_data.get('url', ''))
                self.recorder.log("info", f"âœ… [æ·±åº¦ç ”ç©¶] å·²æ”¶é›† {posts_processed}/{DEEP_RESEARCH_POST_LIMIT} ä¸ªå¸–å­ (ID: {note_id[:8] if note_id else 'unknown'}...)")
            else:
                # è®°å½•è·³è¿‡åŸå› 
                skip_reason = "æ— æ•°æ®" if not post_data else "å®Œå…¨æ— å†…å®¹ï¼ˆæ— æ–‡å­—ã€å›¾ç‰‡ã€è§†é¢‘ã€è¯„è®ºï¼‰"
                self.recorder.log("warning", f"âš ï¸ [æ·±åº¦ç ”ç©¶] è·³è¿‡å¸–å­: {skip_reason} (å°è¯• {attempts})")

            # 7. å…³é—­è¯¦æƒ…é¡µï¼Œè¿”å›æœç´¢ç»“æœé¡µï¼ˆç ”ç©¶æ¨¡å¼ï¼šå¿«é€Ÿå…³é—­ï¼‰
            await asyncio.sleep(random.uniform(0.5, 0.8))  # å‡åŠå»¶è¿Ÿ
            if await self.human.click_element(SELECTORS["btn_close"], "å…³é—­è¯¦æƒ…"):
                self.recorder.log("debug", "ä½¿ç”¨æŒ‰é’®å…³é—­è¯¦æƒ…é¡µ")
            else:
                await self.page.keyboard.press("Escape")
                self.recorder.log("debug", "ä½¿ç”¨ Escape å…³é—­è¯¦æƒ…é¡µ")

            # 8. ç­‰å¾…é®ç½©å±‚å®Œå…¨æ¶ˆå¤±ï¼Œé¿å…æ‹¦æˆªä¸‹ä¸€æ¬¡ç‚¹å‡»
            try:
                await self.page.wait_for_selector(
                    SELECTORS["note_detail_mask"],
                    state="hidden",
                    timeout=5000
                )
                self.recorder.log("debug", "âœ… é®ç½©å±‚å·²æ¶ˆå¤±")
            except Exception as e:
                self.recorder.log("warning", f"âš ï¸ ç­‰å¾…é®ç½©å±‚æ¶ˆå¤±è¶…æ—¶: {e}")
                # å¦‚æœé®ç½©å±‚ä»ç„¶å­˜åœ¨ï¼Œå¼ºåˆ¶ç­‰å¾…æ›´é•¿æ—¶é—´
                await asyncio.sleep(1.0)

            # 9. å¦‚æœè¿˜éœ€è¦æ›´å¤šå¸–å­ï¼Œå¶å°”æ»šåŠ¨é¡µé¢åŠ è½½æ–°å†…å®¹
            if posts_processed < DEEP_RESEARCH_POST_LIMIT and posts_processed % 3 == 0:
                self.recorder.log("info", "ğŸ“œ [æ·±åº¦ç ”ç©¶] æ»šåŠ¨åŠ è½½æ›´å¤šå¸–å­...")
                await self.human.human_scroll(random.randint(800, 1200))
                await asyncio.sleep(random.uniform(1.0, 1.5))  # å‡åŠå»¶è¿Ÿ

        # ä¿å­˜ç ”ç©¶æ•°æ®
        if research_data:
            data_filename = self.output_dir / f"research_data_{search_term}.json"
            with open(data_filename, "w", encoding="utf-8") as f:
                serializable_data = []
                for item in research_data:
                    serializable_item = item.copy()
                    if 'video_local_path' in serializable_item and isinstance(serializable_item['video_local_path'], Path):
                        serializable_item['video_local_path'] = str(serializable_item['video_local_path'])
                    serializable_data.append(serializable_item)
                json.dump(serializable_data, f, ensure_ascii=False, indent=4)
            self.recorder.log("info", f"ğŸ’¾ [æ·±åº¦ç ”ç©¶] åŸå§‹æ•°æ®å·²ä¿å­˜: {data_filename}")

            report = await self._generate_report(research_data)
            await self._save_report(report, search_term)
        else:
            self.recorder.log("warning", "âš ï¸ [æ·±åº¦ç ”ç©¶] æœªæ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")

        self.recorder.log("info", f"ğŸ‰ [æ·±åº¦ç ”ç©¶] æ·±åº¦ç ”ç©¶å®Œæˆï¼å…±æ”¶é›† {len(research_data)} ä¸ªå¸–å­")


    async def _perform_search(self, keyword: str):
        self.recorder.log("info", f"ğŸ” [æœç´¢] å¼€å§‹æœç´¢å…³é”®è¯: '{keyword}'")

        try:
            # 1. ç¡®ä¿åœ¨å°çº¢ä¹¦é¦–é¡µ
            if "xiaohongshu.com" not in self.page.url or "/search_result" in self.page.url:
                self.recorder.log("info", "ğŸ” [æœç´¢] å¯¼èˆªåˆ°å°çº¢ä¹¦é¦–é¡µ...")
                await self.page.goto(BASE_URL)
                await asyncio.sleep(1)

            # 2. ç‚¹å‡»æœç´¢æ¡†
            await self.human.click_element(SELECTORS["search_input"], "æœç´¢æ¡†")
            await asyncio.sleep(random.uniform(0.5, 1.0))

            # 3. æ¸…ç©ºå¹¶è¾“å…¥å…³é”®è¯
            await self.page.locator(SELECTORS["search_input"]).clear()
            for char in keyword:
                await self.page.keyboard.type(char, delay=random.randint(50, 150))

            # 4. æäº¤æœç´¢
            self.recorder.log("info", f"ğŸ” [æœç´¢] æäº¤æœç´¢: '{keyword}'")
            await self.page.keyboard.press("Enter")

            # 5. ç­‰å¾…æœç´¢ç»“æœé¡µé¢åŠ è½½ï¼ˆå…³é”®ï¼ï¼‰
            await self.page.wait_for_load_state("networkidle", timeout=15000)

            # 6. é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿ç¬”è®°å¡ç‰‡æ¸²æŸ“å®Œæˆ
            await asyncio.sleep(3)

            self.recorder.log("info", f"âœ… [æœç´¢] æœç´¢å®Œæˆï¼Œå½“å‰URL: {self.page.url}")
        except Exception as e:
            self.recorder.log("error", f"âŒ [æœç´¢] æœç´¢å¤±è´¥ '{keyword}': {e}")
            raise


    async def _transcribe_video(self, video_local_path: Path) -> str:
        """Sends a local video file to the ASR server for transcription."""
        if not ASR_SERVER_URL:
            self.recorder.log("warning", "ASR_SERVER_URL is not configured. Skipping video transcription.")
            return ""

        if not video_local_path.exists():
            self.recorder.log("warning", f"Video file not found for transcription: {video_local_path}")
            return ""

        self.recorder.log("info", f"Sending {video_local_path.name} to ASR server for transcription (language=zh)...")
        try:
            async with httpx.AsyncClient(timeout=300.0) as client: # Increased timeout for large files
                with open(video_local_path, "rb") as f:
                    files = {'file': (video_local_path.name, f, 'audio/mpeg')}
                    data = {'language': 'zh', 'task': 'transcribe'}  # å¼ºåˆ¶ä½¿ç”¨ä¸­æ–‡
                    response = await client.post(ASR_SERVER_URL, files=files, data=data)
                response.raise_for_status() # Raise an exception for HTTP errors
                
                result = response.json()
                transcription = result.get("text", "")
                if transcription:
                    self.recorder.log("info", f"ASR successful for {video_local_path.name}: {transcription[:50]}...")
                else:
                    self.recorder.log("warning", f"ASR returned empty transcription for {video_local_path.name}.")
                return transcription
        except httpx.RequestError as exc:
            self.recorder.log("error", f"ASR request error for {video_local_path.name}: {exc}")
        except httpx.HTTPStatusError as exc:
            self.recorder.log("error", f"ASR HTTP error for {video_local_path.name} - {exc.response.status_code}: {exc.response.text}")
        except Exception as e:
            self.recorder.log("error", f"Unexpected error during ASR for {video_local_path.name}: {e}")
        return ""

    async def _download_image(self, url: str) -> bytes | None:
        """ä»URLå¼‚æ­¥ä¸‹è½½å›¾ç‰‡"""
        if not url:
            return None
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=30) as response:
                    response.raise_for_status()
                    return await response.read()
        except aiohttp.ClientError as e:
            self.recorder.log("warning", f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ {url}: {e}")
            return None
        except asyncio.TimeoutError:
            self.recorder.log("warning", f"å›¾ç‰‡ä¸‹è½½è¶…æ—¶ {url}")
            return None

    async def _perform_ocr_on_bytes(self, image_bytes: bytes) -> list[str]:
        """å¯¹å›¾ç‰‡å­—èŠ‚æ•°æ®æ‰§è¡Œ OCR"""
        if not self.ocr_engine or not image_bytes:
            return []
        
        try:
            # RapidOCR expects a file path or a numpy array.
            # We can convert bytes to a file-like object in memory.
            # A more direct approach might be to save to a temp file and pass the path,
            # but given the prompt, let's try to keep it in memory if possible.
            # RapidOCR also accepts PIL Image.
            from PIL import Image
            img = Image.open(io.BytesIO(image_bytes))
            
            # The demo showed engine("filepath.webp") which returns result.txts
            # If we pass PIL Image, it might return a different structure.
            # Let's assume it still returns a structure from which txts can be extracted.
            ocr_results = await asyncio.to_thread(self.ocr_engine, img)
            
            if ocr_results and hasattr(ocr_results, 'txts'):
                return ocr_results.txts
            elif isinstance(ocr_results, list) and all(isinstance(item, tuple) for item in ocr_results):
                # RapidOCR's default output when directly calling engine(image) is often
                # a list of tuples: (bbox, text, score)
                return [item[1] for item in ocr_results]
            else:
                self.recorder.log("warning", f"OCR ç»“æœæ ¼å¼æœªçŸ¥: {ocr_results}")
                return []

        except Exception as e:
            self.recorder.log("error", f"OCR æ‰§è¡Œå¼‚å¸¸: {e}")
            return []

    async def _extract_content_from_page(self):
        """æå–å¸–å­å®Œæ•´å†…å®¹ï¼šæ ‡é¢˜ã€æ­£æ–‡ã€ä½œè€…ã€å›¾ç‰‡ã€è§†é¢‘ã€è¯„è®º"""
        detail = {
            "url": self.page.url,  # æ·»åŠ å½“å‰é¡µé¢URL
            "title": "", "content": "",
            "author": "",  # æ–°å¢ï¼šåšä¸»åå­—
            "author_avatar": "",  # æ–°å¢ï¼šåšä¸»å¤´åƒ
            "publish_date": "",  # æ–°å¢ï¼šå‘å¸ƒæ—¥æœŸ
            "image_urls": [], "video_url": "", "video_local_path": "", "media_type": "image",
            "comments": [],
            "ocr_results": [],  # Placeholder for OCR, changed to list
            "asr_results": ""   # Placeholder for ASR
        }
        try:
            if await self.page.locator(SELECTORS["detail_title"]).count() > 0:
                detail["title"] = await self.page.locator(SELECTORS["detail_title"]).inner_text()

            if await self.page.locator(SELECTORS["detail_desc"]).count() > 0:
                detail["content"] = await self.page.locator(SELECTORS["detail_desc"]).inner_text()

            # æå–ä½œè€…ä¿¡æ¯ï¼ˆä½¿ç”¨.firsté¿å…å¤šä¸ªåŒ¹é…ï¼‰
            author_locator = self.page.locator(SELECTORS["detail_author"]).first
            if await author_locator.count() > 0:
                try:
                    detail["author"] = await author_locator.inner_text()
                except:
                    detail["author"] = ""
            
            # æå–ä½œè€…å¤´åƒ
            avatar_locator = self.page.locator(SELECTORS["author_avatar"]).first
            if await avatar_locator.count() > 0:
                try:
                    detail["author_avatar"] = await avatar_locator.get_attribute("src") or ""
                except:
                    detail["author_avatar"] = ""

            # æå–å‘å¸ƒæ—¥æœŸ
            detail["publish_date"] = await self._extract_publish_date()

            detail["image_urls"] = await self._extract_images()

            # æå–å¹¶ä¸‹è½½è§†é¢‘
            video_info = await self._extract_video()
            detail["video_url"] = video_info.get("video_url", "")
            detail["video_local_path"] = video_info.get("local_path", "")
            detail["media_type"] = "video" if detail["video_url"] else "image"

            # æ‰§è¡Œ ASR è½¬å½•ï¼ˆå¦‚æœæœ‰è§†é¢‘ï¼‰
            if detail["video_local_path"] and os.path.exists(detail["video_local_path"]):
                detail["asr_results"] = await self._transcribe_video(Path(detail["video_local_path"]))

            # OCR å¤„ç†å›¾ç‰‡
            if detail["image_urls"] and self.ocr_engine:
                all_ocr_texts = []
                self.recorder.log("info", f"âœ¨ [OCR] å¼€å§‹å¤„ç† {len(detail['image_urls'])} å¼ å›¾ç‰‡...")
                for img_url in detail["image_urls"]:
                    image_bytes = await self._download_image(img_url)
                    if image_bytes:
                        ocr_texts = await self._perform_ocr_on_bytes(image_bytes)
                        if ocr_texts:
                            all_ocr_texts.extend(ocr_texts)
                            self.recorder.log("debug", f"ğŸ“¸ [OCR] ä»å›¾ç‰‡ '{img_url[:50]}...' æå–æ–‡æœ¬: {ocr_texts[:3]}...")
                if all_ocr_texts:
                    detail["ocr_results"] = all_ocr_texts
                    self.recorder.log("info", f"âœ… [OCR] ä» {len(detail['image_urls'])} å¼ å›¾ç‰‡ä¸­æå–åˆ° {len(all_ocr_texts)} æ¡OCRæ–‡æœ¬ã€‚")
                else:
                    self.recorder.log("info", f"â„¹ï¸ [OCR] æœªèƒ½ä»å›¾ç‰‡ä¸­æå–åˆ°æ–‡æœ¬ã€‚")


            # 1. æ»šåŠ¨åŠ è½½æ›´å¤šä¸€çº§è¯„è®º (æœ€å¤š DEEP_RESEARCH_COMMENT_LIMIT)
            for _ in range(3): # Scroll a few times to get initial comments
                await self._scroll_comment_area()
                await asyncio.sleep(random.uniform(1, 2))

            # 2. å±•å¼€æ‰€æœ‰æŠ˜å çš„äºŒçº§è¯„è®º
            await self._expand_all_replies()
            await asyncio.sleep(random.uniform(1, 2))

            # 3. æå–è¯„è®º
            all_comments = await self._extract_comments()
            detail["comments"] = all_comments[:DEEP_RESEARCH_COMMENT_LIMIT] # Limit comments

            # æå–å¸–å­ID
            note_id = self._extract_note_id_from_url(self.page.url)
            note_id_short = note_id[:8] if note_id else "unknown"

            media_count = len(detail["image_urls"]) if detail["media_type"] == "image" else 1
            content_preview = detail['content'][:30].replace('\n', ' ') if detail['content'] else '(æ— æ­£æ–‡)'
            author_preview = detail['author'][:15] if detail['author'] else '(æœªçŸ¥ä½œè€…)'
            self.recorder.log("info", 
                f"ğŸ“¸ [æŠ“å–å®Œæˆ] å¸–å­ {note_id_short}... | ä½œè€…:{author_preview} | {detail['media_type']}x{media_count} | è¯„è®ºx{len(detail['comments'])} | å†…å®¹: {content_preview}...")

        except Exception as e:
            self.recorder.log("warning", f"å†…å®¹æå–å¼‚å¸¸: {e}")
        return detail

    async def _extract_images(self):
        """ä»è¯¦æƒ…é¡µDOMæå–æ‰€æœ‰å›¾ç‰‡URL"""
        try:
            return await self.page.evaluate("""
                () => {
                    const urls = new Set();
                    // åœ¨åª’ä½“å®¹å™¨ä¸­æŸ¥æ‰¾å›¾ç‰‡
                    const containers = document.querySelectorAll(
                        '.note-detail-mask .swiper-slide img, ' +
                        '.note-detail-mask .media-container img, ' +
                        '.note-detail-mask [class*="carousel"] img, ' +
                        '.note-detail-mask [class*="slider"] img'
                    );
                    containers.forEach(img => {
                        const src = img.src || img.dataset.src || img.getAttribute('data-src') || '';
                        if (src && (src.includes('xhscdn') || src.includes('xiaohongshu') || src.includes('sns-'))
                            && !src.includes('avatar') && !src.includes('emoji')) {
                            urls.add(src);
                        }
                    });
                    // å¤‡é€‰ï¼šdetail mask å†…æ‰€æœ‰å¤§å›¾
                    if (urls.size === 0) {
                        document.querySelectorAll('.note-detail-mask img').forEach(img => {
                            const src = img.src || img.dataset.src || '';
                            if (src && (src.includes('xhscdn') || src.includes('xiaohongshu'))
                                && !src.includes('avatar') && !src.includes('emoji')
                                && img.naturalWidth > 100) {
                                urls.add(src);
                            }
                        });
                    }
                    return [...urls];
                }
            """) or []
        except Exception as e:
            self.recorder.log("warning", f"å›¾ç‰‡æå–å¼‚å¸¸: {e}")
            return []

    async def _extract_video(self):
        """
        æå–å¹¶ä¸‹è½½è§†é¢‘
        ä½¿ç”¨ VideoDownloader ä»ç½‘é¡µ __INITIAL_STATE__ æå–è§†é¢‘ä¿¡æ¯å¹¶ä¸‹è½½
        è¿”å›åŒ…å« video_url å’Œ local_path çš„å­—å…¸
        """
        try:
            # æ­¥éª¤1: DOM å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘ç¬”è®°
            is_video = await self.page.evaluate("""
                () => {
                    const noteContainer = document.querySelector('#noteContainer, [data-type="video"]');
                    return noteContainer && noteContainer.getAttribute('data-type') === 'video';
                }
            """)

            if not is_video:
                return {"video_url": "", "local_path": ""}  # ä¸æ˜¯è§†é¢‘ç¬”è®°

            # æ­¥éª¤2: è·å–å½“å‰ URL å’Œ note_id
            current_url = self.page.url
            note_id = self._extract_note_id_from_url(current_url)
            note_id_short = note_id[:8] if note_id else "unknown"
            self.recorder.log("info", f"ğŸ“¹ [è§†é¢‘ä¸‹è½½] å¸–å­ {note_id_short}... æ£€æµ‹åˆ°è§†é¢‘ï¼Œå¼€å§‹æå–...")

            # æ­¥éª¤3: æå–è§†é¢‘ä¿¡æ¯å¹¶ä¸‹è½½
            result = await self.video_downloader.extract_and_download(current_url)

            if result:
                self.recorder.log("info", f"âœ… [è§†é¢‘ä¸‹è½½] å¸–å­ {note_id_short}... ä¸‹è½½æˆåŠŸ")
                self.recorder.log("info", f"   è§†é¢‘URL: {result['video_url'][:50]}...")
                self.recorder.log("info", f"   ä¿å­˜è·¯å¾„: {result['local_path']}")
                return {
                    "video_url": result["video_url"],
                    "local_path": result["local_path"],
                }
            else:
                self.recorder.log("warning", f"âš ï¸ [è§†é¢‘ä¸‹è½½] å¸–å­ {note_id_short}... æå–æˆ–ä¸‹è½½å¤±è´¥")
                return {"video_url": "", "local_path": ""}

        except Exception as e:
            note_id = self._extract_note_id_from_url(self.page.url if self.page else "")
            note_id_short = note_id[:8] if note_id else "unknown"
            self.recorder.log("error", f"âŒ [è§†é¢‘ä¸‹è½½] å¸–å­ {note_id_short}... å¼‚å¸¸: {e}")
            return {"video_url": "", "local_path": ""}

    async def _extract_publish_date(self) -> str:
        """ä»è¯¦æƒ…é¡µæå–å‘å¸ƒæ—¥æœŸ

        Returns:
            å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¦‚ "æ˜¨å¤© 14:53 ç¦å»º"ï¼‰
            å¦‚æœæå–å¤±è´¥ï¼Œè¿”å› "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
        """
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨ï¼ˆå®¹é”™ï¼‰
            selectors = [
                '.bottom-container .date',
                '.notedetail-menu + .date',
                '[class*="bottom"] .date'
            ]
            for selector in selectors:
                element = self.page.locator(selector).first
                if await element.count() > 0:
                    date_text = await element.inner_text()
                    if date_text.strip():
                        return date_text.strip()
            return "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
        except Exception as e:
            self.recorder.log("warning", f"æ—¥æœŸæå–å¼‚å¸¸: {e}")
            return "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"

    def _extract_note_id_from_url(self, url: str) -> str:
        """ä» URL ä¸­æå– note ID

        Args:
            url: å¸–å­ URLï¼ˆå¦‚ https://www.xiaohongshu.com/explore/690b1814...)

        Returns:
            note IDï¼ˆå¦‚ 690b1814...ï¼‰ï¼Œæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        match = re.search(r'/explore/([a-f0-9]+)', url)
        return match.group(1) if match else ""

    async def _find_unvisited_note(self, notes):
        """ä»ç¬”è®°åˆ—è¡¨ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœªè®¿é—®çš„ç¬”è®°

        Args:
            notes: å¸–å­å…ƒç´ åˆ—è¡¨

        Returns:
            (target_note, note_id) å…ƒç»„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› (None, None)
        """
        for note in notes:
            href = await note.get_attribute('href')
            note_id = self._extract_note_id_from_url(href or "")
            if note_id and note_id not in self.visited_note_ids:
                return note, note_id
        return None, None

    async def _recover_from_environment_drift(self, search_term: str) -> bool:
        """ç¯å¢ƒåç¦»åçš„æ¢å¤é€»è¾‘

        å½“æ£€æµ‹åˆ°ä¸åœ¨ search_result é¡µé¢æ—¶ï¼Œå¯¼èˆªå›ä¸»é¡µå¹¶é‡æ–°æœç´¢

        Args:
            search_term: æœç´¢å…³é”®è¯

        Returns:
            True è¡¨ç¤ºæ¢å¤æˆåŠŸï¼ŒFalse è¡¨ç¤ºæ¢å¤å¤±è´¥
        """
        try:
            self.recorder.log("warning", f"âš ï¸ [ç¯å¢ƒåç¦»] å½“å‰URL: {self.page.url}")
            self.recorder.log("info", "ğŸ”„ [æ¢å¤] å¯¼èˆªå›ä¸»é¡µå¹¶é‡æ–°æœç´¢...")

            # å¯¼èˆªå›ä¸»é¡µ
            await self.page.goto("https://www.xiaohongshu.com/explore")
            await asyncio.sleep(2)

            # é‡æ–°æ‰§è¡Œæœç´¢
            await self._perform_search(search_term)

            self.recorder.log("info", "âœ… [æ¢å¤] ç¯å¢ƒæ¢å¤æˆåŠŸ")
            return True
        except Exception as e:
            self.recorder.log("error", f"âŒ [æ¢å¤] ç¯å¢ƒæ¢å¤å¤±è´¥: {e}")
            return False

    async def _extract_comments(self):
        """ä»è¯¦æƒ…é¡µDOMæå–å¯è§è¯„è®ºï¼ˆä¸€çº§+äºŒçº§ï¼‰"""
        try:
            return await self.page.evaluate("""
                () => {
                    const results = [];
                    // æŸ¥æ‰¾æ‰€æœ‰ä¸€çº§è¯„è®ºå®¹å™¨
                    const parentComments = document.querySelectorAll('.note-detail-mask .parent-comment');

                    parentComments.forEach(parentItem => {
                        try {
                            // æå–ä¸€çº§è¯„è®º
                            const mainComment = parentItem.querySelector('.comment-item:not(.comment-item-sub)');
                            if (!mainComment) return;

                            const userEl = mainComment.querySelector('.author-wrapper .name, a.name');
                            const user = userEl ? userEl.textContent.trim() : '';

                            const contentEl = mainComment.querySelector('.content .note-text');
                            const content = contentEl ? contentEl.textContent.trim() : '';

                            const likeEl = mainComment.querySelector('.like-wrapper .count');
                            const likesText = likeEl ? likeEl.textContent.trim() : '0';

                            // æå–äºŒçº§è¯„è®ºï¼ˆå­è¯„è®ºï¼‰
                            const sub_comments = [];
                            const replyContainer = parentItem.querySelector('.reply-container');
                            if (replyContainer) {
                                const subItems = replyContainer.querySelectorAll('.comment-item-sub');
                                subItems.forEach(sub => {
                                    const sUserEl = sub.querySelector('.author-wrapper .name, a.name');
                                    const sUser = sUserEl ? sUserEl.textContent.trim() : '';

                                    const sContentEl = sub.querySelector('.content .note-text');
                                    const sContent = sContentEl ? sContentEl.textContent.trim() : '';

                                    if (sContent) {
                                        sub_comments.push({ user: sUser, content: sContent });
                                    }
                                });
                            }

                            if (content) {
                                results.push({
                                    user,
                                    content,
                                    likes: parseInt(likesText.replace(/[^0-9]/g, '')) || 0,
                                    sub_comments
                                });
                            }
                        } catch(e) {
                            console.error('è¯„è®ºæå–é”™è¯¯:', e);
                        }
                    });
                    return results;
                }
            """) or []
        except Exception as e:
            self.recorder.log("warning", f"è¯„è®ºæå–å¼‚å¸¸: {e}")
            return []

    async def _scroll_comment_area(self):
        """æ»šåŠ¨è¯¦æƒ…é¡µå³ä¾§é¢æ¿ï¼ŒåŠ è½½æ›´å¤šè¯„è®º"""
        try:
            scrolled = await self.page.evaluate("""
                () => {
                    const containers = [
                        document.querySelector('.note-detail-mask .interaction-container'),
                        document.querySelector('.note-detail-mask .note-scroller'),
                        document.querySelector('.note-detail-mask [class*="contentContainer"]'),
                        document.querySelector('.note-detail-mask .right-container')
                    ];
                    for (const c of containers) {
                        if (c && c.scrollHeight > c.clientHeight) {
                            c.scrollBy({ top: 500, behavior: 'smooth' });
                            return true;
                        }
                    }
                    return false;
                }
            """)
            if scrolled:
                await asyncio.sleep(random.uniform(0.8, 1.5))
        except Exception:
            pass

    async def _expand_all_replies(self):
        """å±•å¼€æ‰€æœ‰æŠ˜å çš„äºŒçº§è¯„è®ºï¼ˆç‚¹å‡»"å±•å¼€Xæ¡å›å¤"æŒ‰é’®ï¼‰"""
        try:
            expanded_count = await self.page.evaluate("""
                () => {
                    const showMoreButtons = document.querySelectorAll('.note-detail-mask .show-more');
                    let count = 0;
                    showMoreButtons.forEach(btn => {
                        if (btn && btn.textContent.includes('å±•å¼€') && btn.textContent.includes('å›å¤')) {
                            btn.click();
                            count++;
                        }
                    });
                    return count;
                }
            """)
            if expanded_count > 0:
                self.recorder.log("info", f"ğŸ’¬ [è¯„è®º] å±•å¼€äº† {expanded_count} ä¸ªæŠ˜å çš„å›å¤")
                # ç­‰å¾…å±•å¼€çš„è¯„è®ºåŠ è½½
                await asyncio.sleep(random.uniform(1.0, 2.0))
        except Exception as e:
            self.recorder.log("warning", f"å±•å¼€å›å¤å¤±è´¥: {e}")

    async def _generate_report(self, research_data: list[dict]) -> str:
        # Placeholder for LLM report generation
        self.recorder.log("info", "Generating research report using LLM.")
        prompt = self._prepare_llm_prompt(research_data)
        # Assuming llm_client has a method like generate_text
        report = await self.llm_client.generate_text(prompt, model=DEEP_RESEARCH_LLM_MODEL)
        return self._postprocess_report(report, research_data)

    def _postprocess_report(self, report: str, research_data: list[dict]) -> str:
        """
        ç›®æ ‡ï¼šæŠŠ LLM å¸¸è§çš„â€œå¼•ç”¨å†™æ³•â€å¼ºåˆ¶ä¿®æ­£ä¸ºå¯ç‚¹å‡»é“¾æ¥ï¼Œé¿å…å‚è€ƒæ–‡çŒ®/æ­£æ–‡å‡ºç°çº¯æ–‡æœ¬ URL æˆ–åå¼•å·åŒ…è£¹å¼•ç”¨ã€‚
        - å°† `è§[å¸–å­[3]]è¯„è®º` / è§[å¸–å­[3]]è¯„è®º â†’ è§[å¸–å­[3]](URL)è¯„è®º
        - å°† [å¸–å­[3]]ï¼ˆæœªå¸¦é“¾æ¥ï¼‰â†’ [å¸–å­[3]](URL)
        - å°½é‡è·³è¿‡ fenced code blockï¼ˆ```...```ï¼‰ä»¥å…æ±¡æŸ“ä»£ç /mermaid
        """
        if not report or not research_data:
            return report

        idx_to_url: dict[int, str] = {}
        for i, post in enumerate(research_data, 1):
            url = (post.get("url") or "").strip()
            if url:
                idx_to_url[i] = url

        if not idx_to_url:
            return report

        def _fix_line(line: str) -> str:
            # å»æ‰å¼•ç”¨å¤–å±‚åå¼•å·ï¼ˆä»…é’ˆå¯¹â€œè§[å¸–å­[..]]â€è¿™ç±»ç‰‡æ®µï¼‰
            line = re.sub(r"`\s*(è§\s*\[å¸–å­\[(\d+)\]\][^`]*)\s*`", r"\1", line)

            # è§[å¸–å­[3]]è¯„è®º â†’ è§[å¸–å­[3]](URL)è¯„è®º
            def repl_seen(m: re.Match):
                idx = int(m.group(2))
                url = idx_to_url.get(idx)
                if not url:
                    return m.group(0)
                tail = m.group(3) or ""
                return f"è§[å¸–å­[{idx}]]({url}){tail}"

            # è‹¥å·²ç»æ˜¯ Markdown é“¾æ¥ï¼ˆ]åç´§è·Ÿ(ï¼‰ï¼Œåˆ™ä¸é‡å¤æ³¨å…¥ URL
            line = re.sub(r"(è§\s*)\[å¸–å­\[(\d+)\]\](?!\()(è¯„è®º)?", repl_seen, line)

            # [å¸–å­[3]]ï¼ˆæœªå¸¦é“¾æ¥ï¼‰â†’ [å¸–å­[3]](URL)
            def repl_bare(m: re.Match):
                idx = int(m.group(1))
                url = idx_to_url.get(idx)
                if not url:
                    return m.group(0)
                return f"[å¸–å­[{idx}]]({url})"

            line = re.sub(r"\[å¸–å­\[(\d+)\]\](?!\()", repl_bare, line)

            # å‚è€ƒæ–‡çŒ®å¸¸è§å†™æ³•ï¼šé“¾æ¥(URL) â†’ [å¸–å­é“¾æ¥](URL)
            line = re.sub(r"é“¾æ¥\((https?://[^\s)]+)\)", r"[å¸–å­é“¾æ¥](\1)", line)
            return line

        def _convert_mermaid_bar_to_xychart(mermaid_src: str) -> str:
            """
            å°†éæ ‡å‡†çš„
              bar
                title xxx
                x-axis ...
                y-axis ...
                bar "A": 10
            è½¬ä¸º mermaid@10 æ”¯æŒçš„ xychart-betaã€‚
            """
            lines = [ln.rstrip() for ln in mermaid_src.splitlines()]
            # æ‰¾åˆ°é¦–ä¸ªéç©ºè¡Œ
            i0 = next((i for i, ln in enumerate(lines) if ln.strip()), None)
            if i0 is None:
                return mermaid_src
            if lines[i0].strip() != "bar":
                return mermaid_src

            title = ""
            y_label = "æ¬¡æ•°"
            points: list[tuple[str, float]] = []
            for ln in lines[i0 + 1 :]:
                s = ln.strip()
                if not s:
                    continue
                if s.startswith("title"):
                    title = s[len("title") :].strip()
                    continue
                if s.startswith("y-axis"):
                    # y-axis æ¬¡æ•°
                    y_label = s[len("y-axis") :].strip() or y_label
                    continue
                m = re.match(r'^bar\s+"?(.*?)"?\s*:\s*([0-9]+(?:\.[0-9]+)?)\s*$', s)
                if m:
                    points.append((m.group(1), float(m.group(2))))

            if not points:
                return mermaid_src

            labels = [p[0].replace('"', '\\"') for p in points]
            values = [p[1] for p in points]
            y_max = max(values) if values else 0
            y_max_int = int(y_max) if float(y_max).is_integer() else int(y_max) + 1
            if y_max_int <= 0:
                y_max_int = 1

            # è®© y è½´ä¸Šé™æ›´â€œå¥½çœ‹â€
            step = 10
            y_max_int = ((y_max_int + step - 1) // step) * step

            values_str = ", ".join(str(int(v)) if float(v).is_integer() else str(v) for v in values)
            labels_str = ", ".join(f'"{lab}"' for lab in labels)
            title_escaped = title.replace('"', '\\"') if title else "æåŠé¢‘æ¬¡TOP"

            return "\n".join(
                [
                    "xychart-beta",
                    f'    title "{title_escaped}"',
                    f"    x-axis [{labels_str}]",
                    f'    y-axis "{y_label}" 0 --> {y_max_int}',
                    f"    bar [{values_str}]",
                ]
            )

        def _rebuild_references_section() -> str:
            lines: list[str] = ["## å‚è€ƒæ–‡çŒ®", ""]
            for i, post in enumerate(research_data, 1):
                url = (post.get("url") or "").strip()
                title = (post.get("title") or "(æ— æ ‡é¢˜)").strip()
                author = (post.get("author") or "").strip()
                publish_date = (post.get("publish_date") or "").strip()
                author = author if author else "ä½œè€…æœªæä¾›"
                publish_date = publish_date if publish_date else "å‘å¸ƒæ—¥æœŸæœªæä¾›"
                if url:
                    lines.append(f"[{i}] @{author}. ã€Š{title}ã€‹. å°çº¢ä¹¦, {publish_date}. [å¸–å­é“¾æ¥]({url})")
                else:
                    lines.append(f"[{i}] @{author}. ã€Š{title}ã€‹. å°çº¢ä¹¦, {publish_date}. ï¼ˆé“¾æ¥ç¼ºå¤±ï¼‰")
            return "\n".join(lines).rstrip() + "\n"

        raw_lines = report.splitlines()
        out: list[str] = []
        in_fence = False
        fence_lang = ""
        mermaid_buf: list[str] = []

        # å‚è€ƒæ–‡çŒ®æ®µè½æ›¿æ¢ï¼šé‡åˆ° "## å‚è€ƒæ–‡çŒ®" åï¼Œè·³è¿‡ç›´åˆ°ä¸‹ä¸€ä¸ª "## " æˆ– EOF
        i = 0
        while i < len(raw_lines):
            line = raw_lines[i]
            if not in_fence and line.strip() == "## å‚è€ƒæ–‡çŒ®":
                out.append(_rebuild_references_section().rstrip())
                i += 1
                while i < len(raw_lines):
                    nxt = raw_lines[i]
                    if nxt.startswith("## ") and nxt.strip() != "## å‚è€ƒæ–‡çŒ®":
                        break
                    i += 1
                continue

            if line.strip().startswith("```"):
                if not in_fence:
                    in_fence = True
                    fence_lang = line.strip()[3:].strip().lower()
                    out.append(line)
                    if fence_lang == "mermaid":
                        mermaid_buf = []
                    i += 1
                    continue
                else:
                    # fence close
                    if fence_lang == "mermaid" and mermaid_buf is not None:
                        src = "\n".join(mermaid_buf)
                        src2 = _convert_mermaid_bar_to_xychart(src)
                        out.append(src2)
                        mermaid_buf = []
                    out.append(line)
                    in_fence = False
                    fence_lang = ""
                    i += 1
                    continue

            if in_fence and fence_lang == "mermaid":
                mermaid_buf.append(line)
                i += 1
                continue

            if in_fence:
                out.append(line)
                i += 1
                continue

            out.append(_fix_line(line))
            i += 1

        return "\n".join(out).rstrip() + "\n"

    async def _save_report(self, report: str, keyword: str):
        report_filename = self.output_dir / f"research_report_{keyword}.md"
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(report)
        self.recorder.log("info", f"Research report saved to {report_filename}")

        # å‚è€ƒ data/demo.html çš„æ¨¡æ¿æ ·å¼ï¼šåŒæ­¥è¾“å‡ºå¯¹åº” HTML
        try:
            html_filename = self.output_dir / f"research_report_{keyword}.html"
            html_text = render_deep_research_html(
                report,
                title_fallback=f"æ·±åº¦è°ƒç ”æŠ¥å‘Šï¼š{keyword}",
                subtitle=f"åŸºäºæŠ“å–æ•°æ®çš„æ·±åº¦ç ”ç©¶ | å…³é”®è¯ï¼š{keyword}",
                generated_at=datetime.now(),
            )
            with open(html_filename, "w", encoding="utf-8") as f:
                f.write(html_text)
            self.recorder.log("info", f"Research report HTML saved to {html_filename}")
        except Exception as e:
            self.recorder.log("warning", f"HTML æŠ¥å‘Šè¾“å‡ºå¤±è´¥ï¼ˆå·²ä¿ç•™ Markdownï¼‰ï¼š{e}")

    def _prepare_llm_prompt(self, research_data: list[dict]) -> str:
        """
        æ„å»º LLM æç¤ºè¯ï¼šç”Ÿæˆä¸“ä¸šçš„æ•°æ®è°ƒç ”åˆ†ææŠ¥å‘Š
        ç›®æ ‡ï¼šåŸºäºå°çº¢ä¹¦æ•°æ®ç”Ÿæˆæ•°æ®é²œæ˜ã€è®ºè¯ä¸¥è°¨çš„ä¸“ä¸šè°ƒç ”æŠ¥å‘Š
        """
        def _truncate(text: str, limit: int) -> str:
            text = (text or "").strip()
            if len(text) <= limit:
                return text
            return text[:limit] + "â€¦"

        def _safe_list(v):
            return v if isinstance(v, list) else []

        # å…³é”®è¯ï¼šå°½é‡ä» URL è§£æï¼ˆè‹¥å¤±è´¥åˆ™å›é€€ä¸ºâ€œä¸»é¢˜â€ï¼‰
        keyword = "ä¸»é¢˜"
        if research_data:
            url0 = (research_data[0].get("url") or "").strip()
            if "keyword=" in url0:
                keyword = url0.split("keyword=")[-1].split("&")[0] or keyword
                try:
                    from urllib.parse import unquote
                    keyword = unquote(keyword)
                except Exception:
                    pass

        posts_cnt = len(research_data)
        total_comments = sum(len(_safe_list(p.get("comments"))) for p in research_data)
        posts_with_video = sum(1 for p in research_data if (p.get("video_url") or "").strip())
        posts_with_images = sum(1 for p in research_data if len(_safe_list(p.get("image_urls"))) > 0)
        posts_with_asr = sum(1 for p in research_data if (p.get("asr_results") or "").strip())
        posts_with_ocr = sum(1 for p in research_data if len(_safe_list(p.get("ocr_results"))) > 0)
        posts_with_text = sum(1 for p in research_data if (p.get("content") or "").strip())

        # === é¢å¤–ç»Ÿè®¡ï¼šç”¨äºâ€œå›¾è¡¨/å¯¹æ¯”/é‡åŒ–â€è¾“å‡ºï¼ˆé¿å…æ¨¡å‹åªå†™ç©ºæ´è®ºè¿°ï¼‰ ===
        def _collect_text(post: dict) -> str:
            parts: list[str] = []
            for k in ("title", "content", "asr_results"):
                v = (post.get(k) or "").strip()
                if v:
                    parts.append(v)
            ocr = _safe_list(post.get("ocr_results"))
            if ocr:
                parts.append(" ".join([str(x) for x in ocr if str(x).strip()]))
            for c in _safe_list(post.get("comments")):
                cv = (c.get("content") or "").strip()
                if cv:
                    parts.append(cv)
            return "\n".join(parts)

        all_text = "\n".join([_collect_text(p) for p in research_data])

        # ç®€æ˜“â€œçŸ­è¯­â€æŠ½å–ï¼šç”¨ä¸­æ–‡è¿ç»­ä¸²è¿‘ä¼¼ï¼ˆä¸ä¾èµ–å¤–éƒ¨åˆ†è¯åº“ï¼‰
        import collections

        stop = {
            "è¿™ä¸ª", "ä¸€ä¸ª", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å°±æ˜¯", "å› ä¸º", "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶å", "çœŸçš„", "æ„Ÿè§‰", "æ¯”è¾ƒ",
            "å¦‚æœ", "è¿˜æ˜¯", "å¯ä»¥", "ä¸æ˜¯", "æ²¡æœ‰", "å¾ˆå¤š", "ç‰¹åˆ«", "ä»¥åŠ", "ä¸€äº›", "è¿™ç§", "é‚£ç§", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ",
            "æ—¶å€™", "ç°åœ¨", "å·²ç»", "ä¸ä¼š", "å¯èƒ½", "éœ€è¦", "è§‰å¾—", "é—®é¢˜", "å†…å®¹", "è¯„è®º", "å¸–å­", "å°çº¢ä¹¦", "æ˜¥æ™š",
        }
        tokens = [t for t in re.findall(r"[\u4e00-\u9fff]{2,6}", all_text) if t not in stop]
        term_counter = collections.Counter(tokens)

        # term -> å‡ºç°åœ¨å“ªäº›å¸–å­ï¼ˆæœ€å¤šç»™ 5 ä¸ªç´¢å¼•ï¼Œæ–¹ä¾¿æ¨¡å‹å¼•ç”¨ï¼‰
        term_posts: dict[str, list[int]] = {}
        for term, _ in term_counter.most_common(40):
            posts_idx = []
            for idx, post in enumerate(research_data, 1):
                if term in _collect_text(post):
                    posts_idx.append(idx)
                if len(posts_idx) >= 5:
                    break
            term_posts[term] = posts_idx

        top_terms = term_counter.most_common(15)
        top_terms_table = "\n".join(
            ["| çŸ­è¯­ | æåŠæ¬¡æ•° | ä¸»è¦æ¥æºå¸–å­ |", "|---|---:|---|"]
            + [
                f"| {term} | {cnt} | {', '.join([f'å¸–å­[{i}]' for i in term_posts.get(term, [])]) or 'â€”'} |"
                for term, cnt in top_terms
            ]
        )

        # è¯„è®ºäº’åŠ¨å¼ºåº¦ï¼šæ¯å¸–è¯„è®ºæ•°ã€ç‚¹èµTop
        per_post_stats_rows = []
        for i, post in enumerate(research_data, 1):
            comments = _safe_list(post.get("comments"))
            like_max = 0
            if comments:
                like_max = max(int(c.get("likes") or 0) for c in comments)
            per_post_stats_rows.append(
                f"| å¸–å­[{i}] | {len((post.get('content') or '').strip())} | {len(comments)} | {like_max} | {'è§†é¢‘' if (post.get('video_url') or '').strip() else 'å›¾æ–‡/å›¾ç‰‡'} |"
            )
        per_post_stats_table = "\n".join(
            ["| å¸–å­ | æ­£æ–‡å­—æ•°(ç²—ç•¥) | è¯„è®ºæ•° | è¯„è®ºæœ€é«˜èµ | å½¢æ€ |", "|---|---:|---:|---:|---|"]
            + per_post_stats_rows[: min(20, len(per_post_stats_rows))]
            + ([f"| â€¦ | â€¦ | â€¦ | â€¦ | â€¦ |"] if len(per_post_stats_rows) > 20 else [])
        )

        # ç»“æ„åŒ–è¯æ®åŒ…ï¼šè®©æ¨¡å‹æ›´å®¹æ˜“â€œå¼•ç”¨è¯æ®â€è€Œä¸æ˜¯å¤è¿°å…¨æ–‡
        evidence_blocks: list[str] = []
        for i, post in enumerate(research_data, 1):
            comments = _safe_list(post.get("comments"))
            top_comments = sorted(comments, key=lambda c: int(c.get("likes") or 0), reverse=True)[:8]

            top_comments_md = "\n".join(
                [
                    f"- ï¼ˆğŸ‘{int(c.get('likes') or 0)}ï¼‰**{(c.get('user') or 'åŒ¿å').strip()}**ï¼š{_truncate(c.get('content') or '', 160)}"
                    for c in top_comments
                    if (c.get("content") or "").strip()
                ]
            ).strip()

            evidence_blocks.append(
                "\n".join(
                    [
                        f"### å¸–å­[{i}]",
                        f"- URLï¼š{post.get('url', 'N/A')}",
                        f"- æ­£æ–‡/å¼•ç”¨é“¾æ¥ï¼ˆå¿…é¡»ç”¨äºæŠ¥å‘Šå¼•ç”¨ï¼‰ï¼š[å¸–å­[{i}]]({post.get('url', 'N/A')})",
                        f"- æ ‡é¢˜ï¼š{(post.get('title') or '(æ— æ ‡é¢˜)').strip()}",
                        f"- ä½œè€…ï¼š{(post.get('author') or '(æœªçŸ¥ä½œè€…)').strip()}",
                        f"- å‘å¸ƒæ—¥æœŸï¼š{(post.get('publish_date') or '(æœªçŸ¥)').strip()}",
                        f"- åª’ä½“ï¼š{'è§†é¢‘' if (post.get('video_url') or '').strip() else 'å›¾æ–‡/å›¾ç‰‡'}ï¼ˆå›¾ç‰‡{len(_safe_list(post.get('image_urls')))}å¼ ï¼‰",
                        f"- æ­£æ–‡æ‘˜å½•ï¼š{_truncate(post.get('content') or '', 420) or '(æ— æ­£æ–‡)'}",
                        f"- ASRæ‘˜å½•ï¼š{_truncate(post.get('asr_results') or '', 420) or '(æ— )'}",
                        f"- OCRæ‘˜å½•ï¼š{_truncate(' '.join(_safe_list(post.get('ocr_results'))), 420) or '(æ— )'}",
                        f"- è¯„è®ºæ•°ï¼š{len(comments)}",
                        f"- Topè¯„è®ºï¼š\n{top_comments_md if top_comments_md else '(æ— å¯ç”¨è¯„è®ºæ‘˜å½•)'}",
                    ]
                )
            )

        prompt = f"""ä½ æ˜¯ä¸€ä½**èµ„æ·±ç”¨æˆ·ç ”ç©¶/è¡Œä¸šåˆ†æå¸ˆ**ã€‚ä½ å°†åŸºäºâ€œè¯æ®åŒ…â€æ’°å†™ä¸€ä»½**æ·±åº¦è°ƒç ”æŠ¥å‘Šï¼ˆMarkdownï¼‰**ã€‚

## ç ”ç©¶ä¸»é¢˜
{keyword}

## æ•°æ®æ ·æœ¬æ¦‚å†µï¼ˆå¿…é¡»åœ¨æŠ¥å‘Šä¸­å¤è¿°å¹¶ç”¨äºè®¡ç®—å£å¾„ï¼‰
- æ ·æœ¬ï¼š{posts_cnt} ç¯‡å¸–å­
- è¯„è®ºæ€»é‡ï¼ˆæŠ“å–åˆ°çš„å¯è§è¯„è®ºï¼‰ï¼š{total_comments} æ¡
- å¸–å­å½¢æ€ï¼šå«è§†é¢‘ {posts_with_video} / å«å›¾ç‰‡ {posts_with_images}
- å¯ç”¨æ–‡æœ¬ï¼šæ­£æ–‡å¯ç”¨ {posts_with_text} / ASRå¯ç”¨ {posts_with_asr} / OCRå¯ç”¨ {posts_with_ocr}
- ç ”ç©¶æ—¶é—´ï¼š{datetime.now().strftime("%Y-%m-%d")}

## å†™ä½œåè®®ï¼ˆæ·±åº¦ç ”ç©¶é£æ ¼ï¼Œå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰
1. **è¯æ®é“¾ä¼˜å…ˆ**ï¼šæ‰€æœ‰ç»“è®ºå¿…é¡»è½åˆ°â€œå¸–å­[X]â€æˆ–â€œå¸–å­[X]çš„è¯„è®ºâ€è¯æ®ï¼›æ— æ³•è¯å®æ—¶å¿…é¡»å†™â€œè¯æ®ä¸è¶³/æ ·æœ¬å¤–æ¨é£é™©â€ã€‚
2. **é‡åŒ–å£å¾„æ¸…æ™°**ï¼šæ‰€æœ‰æ¯”ä¾‹/é¢‘æ¬¡è¦è¯´æ˜åˆ†æ¯ï¼ˆä¾‹å¦‚â€œåœ¨ {posts_cnt} ç¯‡å¸–å­ä¸­ï¼Œæœ‰ 8 ç¯‡æåŠâ€¦å æ¯” 40%â€ï¼‰ã€‚
3. **åä¾‹/åˆ†æ­§ä¸å¯ç¼º**ï¼šæ¯ä¸ªå…³é”®ç»“è®ºè‡³å°‘ç»™å‡º 1 ä¸ªåä¾‹æˆ–å¯¹ç«‹è§‚ç‚¹ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆå‡ºç°åˆ†æ­§ï¼ˆäººç¾¤/åœºæ™¯/æˆæœ¬/è®¤çŸ¥å·®å¼‚ï¼‰ã€‚
4. **ä¸ç¡®å®šæ€§ä¸å±€é™**ï¼šå•åˆ—ç« èŠ‚å†™å‡ºæ ·æœ¬åå·®ã€æŠ“å–ç¼ºå¤±ï¼ˆä¾‹å¦‚ç™»å½•é™åˆ¶ã€è¯„è®ºå±•ç¤ºé™åˆ¶ï¼‰ã€OCR/ASRå™ªå£°ç­‰ã€‚
5. **é«˜å¯†åº¦å¼•ç”¨**ï¼šæ¯ä¸ªäºŒçº§æ ‡é¢˜ï¼ˆ`##`ï¼‰è‡³å°‘åŒ…å« 3 å¤„å¼•ç”¨ï¼ˆä¾‹å¦‚ï¼šè§[å¸–å­[3]](URL)ã€å¸–å­[7]è¯„è®ºï¼‰ï¼›å…¨æ–‡å¼•ç”¨æ•°é‡è‡³å°‘ä¸º {max(12, posts_cnt)} å¤„ã€‚
6. **å¯æ“ä½œ**ï¼šå»ºè®®å¿…é¡»â€œåŠ¨ä½œ+é€‚ç”¨äººç¾¤+è§¦å‘æ¡ä»¶+é£é™©æç¤º+è¯æ®å¼•ç”¨â€ï¼Œé¿å…æ³›æ³›è€Œè°ˆã€‚
7. **è¾“å‡ºå¿…é¡»æ˜¯ Markdown**ï¼Œå¹¶è‡³å°‘åŒ…å«ï¼š
   - Mermaid å›¾ **è‡³å°‘ 3 ä¸ª**ï¼šåˆ†åˆ«ç”¨äºï¼ˆ1ï¼‰è§‚ç‚¹/æƒ…ç»ªåˆ†å¸ƒï¼ˆpie æˆ– barï¼‰ï¼Œï¼ˆ2ï¼‰æåŠé¢‘æ¬¡TOPï¼ˆbar æˆ– xychart-betaï¼‰ï¼Œï¼ˆ3ï¼‰ç”¨æˆ·å†³ç­–è·¯å¾„ï¼ˆflowchartï¼‰
   - è¡¨æ ¼ **è‡³å°‘ 4 ä¸ª**ï¼šæ ·æœ¬æ¦‚è§ˆè¡¨ã€å¯¹æ¯”åˆ†æè¡¨ã€é£é™©æ¸…å•è¡¨ã€è¡ŒåŠ¨å»ºè®®çŸ©é˜µè¡¨
8. **å¼•ç”¨ä¸é“¾æ¥æ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼ï¼‰**ï¼š
   - **ç¦æ­¢**ä½¿ç”¨åå¼•å·åŒ…è£¹å¼•ç”¨ï¼ˆä¾‹å¦‚ä¸è¦å†™ï¼š`è§[å¸–å­[3]]è¯„è®º`ï¼‰
   - æ­£æ–‡å¼•ç”¨ï¼šå¿…é¡»ç”¨å¯ç‚¹å‡»é“¾æ¥ï¼Œä¾‹å¦‚ `è§[å¸–å­[3]](https://...)` æˆ– `ï¼ˆæ¥æºï¼šè§[å¸–å­[3]](URL) è¯„è®ºï¼‰`
   - å‚è€ƒæ–‡çŒ®ï¼šå¿…é¡»æ˜¯ Markdown è¶…é“¾æ¥ï¼Œç¦æ­¢è¾“å‡ºçº¯æ–‡æœ¬ URL

## å¿«é€Ÿç»Ÿè®¡æ‘˜è¦ï¼ˆå¿…é¡»åœ¨æ­£æ–‡ä¸­å¼•ç”¨å¹¶ç”¨å›¾è¡¨/è¡¨æ ¼å±•å¼€ï¼‰
### é«˜é¢‘çŸ­è¯­ï¼ˆæ¥è‡ªæŠ“å–æ–‡æœ¬çš„ç²—ç²’åº¦ç»Ÿè®¡ï¼‰
{top_terms_table}

### å¸–å­å±‚é¢çš„äº’åŠ¨ä¸å½¢æ€ï¼ˆç”¨äºå¯¹æ¯”åˆ†æï¼‰
{per_post_stats_table}

## æŠ¥å‘Šç»“æ„ï¼ˆè¯·æŒ‰æ­¤é¡ºåºä¸æ ‡é¢˜å±‚çº§è¾“å‡ºï¼Œä¾¿äºåç»­ HTML ç›®å½•ç”Ÿæˆï¼‰
# æ·±åº¦è°ƒç ”æŠ¥å‘Šï¼š{keyword}

## æ‰§è¡Œæ‘˜è¦
- 3-5 æ¡â€œç»“è®ºå…ˆè¡Œâ€çš„å…³é”®å‘ç°ï¼ˆæ¯æ¡å«é‡åŒ–ä¸å¼•ç”¨ï¼šå¸–å­[X]â€¦ï¼‰
- 3 æ¡æœ€é‡è¦å»ºè®®ï¼ˆå¯æ‰§è¡Œï¼‰

## ç ”ç©¶è®¾è®¡ä¸æ–¹æ³•
- æ•°æ®æ¥æº/é‡‡é›†æ–¹å¼/æ ·æœ¬è¯´æ˜
- åˆ†ææ¡†æ¶ï¼ˆä½ é‡‡ç”¨çš„åˆ†ç±»å£å¾„ï¼šéœ€æ±‚/åŠ¨æœº/é¡¾è™‘/å†³ç­–å› å­â€¦ï¼‰

## æ•°æ®æ¦‚è§ˆä¸æ ·æœ¬ç”»åƒ
- æ ·æœ¬ç»“æ„ï¼ˆå›¾æ–‡/è§†é¢‘ã€å†…å®¹å¯†åº¦ã€è¯„è®ºæ´»è·ƒåº¦ï¼‰
- å¯èƒ½çš„äººç¾¤çº¿ç´¢ï¼ˆä»å†…å®¹ä¸è¯„è®ºæ¨æ–­ï¼Œä½†è¦å†™â€œæ¨æ–­â€å¹¶ç»™è¯æ®ï¼‰

## æ ¸å¿ƒå‘ç°ï¼ˆåˆ† 3-6 ä¸ªå°èŠ‚ï¼‰
æ¯ä¸ªå°èŠ‚å¿…é¡»åŒ…å«ï¼š
- å°ç»“è®ºï¼ˆ1 å¥è¯ï¼‰
- è¯æ®ï¼šå¼•ç”¨å¸–å­[X]ã€è¯„è®ºæ‘˜å½•ï¼ˆçŸ­å¥å³å¯ï¼‰
- é‡åŒ–ï¼šé¢‘æ¬¡/å æ¯”/æ’åº
- åä¾‹/åˆ†æ­§ï¼šè‡³å°‘ 1 ä¸ª

## è§‚ç‚¹åˆ†å¸ƒä¸äº‰è®®ç‚¹ï¼ˆå« Mermaidï¼‰
å¿…é¡»è¾“å‡º 2 ä¸ª Mermaid å›¾ï¼š
- å›¾1ï¼šè§‚ç‚¹/æƒ…ç»ª/æ€åº¦åˆ†å¸ƒï¼ˆpie æˆ– barï¼Œå¿…é¡»æœ‰æ•°å€¼ï¼‰
- å›¾2ï¼šæåŠé¢‘æ¬¡TOP10ï¼ˆ**å¿…é¡»ç”¨ xychart-beta**ï¼Œæ•°æ®æºå¯ç”¨â€œé«˜é¢‘çŸ­è¯­è¡¨â€æˆ–ä½ åŸºäºè¯æ®è®¡ç®—çš„ç»Ÿè®¡ï¼‰

## ç”¨æˆ·å£°éŸ³ï¼ˆVoCï¼‰
- Top è¯‰æ±‚/Top é¡¾è™‘/Top è¯¯åŒºï¼ˆåˆ†åˆ«ç»™å¼•ç”¨ï¼‰
- å…¸å‹åŸè¯æ‘˜å½•ï¼ˆæ³¨æ˜æ¥æºï¼šå¸–å­[X]è¯„è®ºï¼‰

## é£é™©ã€å±€é™ä¸å¤–æ¨è¾¹ç•Œ
- æ ·æœ¬åå·®/æŠ“å–ç¼ºå¤±/OCR-ASR å™ªå£°
- ç»“è®ºé€‚ç”¨èŒƒå›´ä¸ä¸é€‚ç”¨èŒƒå›´

## è¡ŒåŠ¨å»ºè®®ï¼ˆåˆ†äººç¾¤/åˆ†åœºæ™¯ï¼‰
å»ºè®®é‡‡ç”¨è¡¨æ ¼å‘ˆç°ï¼Œå¹¶åŒ…å«â€œé€‚ç”¨äººç¾¤ã€è§¦å‘æ¡ä»¶ã€æ¨èåŠ¨ä½œã€é£é™©æç¤ºã€è¯æ®å¼•ç”¨â€ã€‚

## å‚è€ƒæ–‡çŒ®ï¼ˆå¿…é¡»è¦†ç›–å…¨éƒ¨ {posts_cnt} ç¯‡å¸–å­ï¼‰
- æ ¼å¼ç¤ºä¾‹ï¼ˆå¿…é¡»å¯ç‚¹å‡»ï¼‰ï¼š
  - `[1] @ä½œè€…. ã€Šæ ‡é¢˜ã€‹. å°çº¢ä¹¦, å‘å¸ƒæ—¥æœŸ. [å¸–å­é“¾æ¥](URL)`
  - æ­£æ–‡å¼•ç”¨ä¹Ÿå»ºè®®ç”¨åŒä¸€ URLï¼š`[å¸–å­[1]](URL)`

---

## è¯æ®åŒ…ï¼ˆåªè®¸å¼•ç”¨ï¼Œä¸è¦åœ¨æŠ¥å‘Šé‡Œå¤å†™å…¨æ–‡ï¼‰
{chr(10).join(evidence_blocks)}
"""

        return prompt

# Example usage (for testing purposes)
async def main():
    # This requires a running Chrome instance with remote debugging on port 9222
    # and a valid Zhipu AI Key (or Kimi if LLMClient is adapted)
    
    # Temporarily set some settings for testing
    from unittest.mock import MagicMock
    browser_manager = MagicMock(spec=BrowserManager)
    browser_manager.page = MagicMock(spec=Page) # Mock the page object
    
    # Mock LLMClient to return dummy text
    llm_client = MagicMock(spec=LLMClient)
    llm_client.generate_text.return_value = "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚"

    research_agent = ResearchAgent(browser_manager, llm_client)
    await research_agent.run_deep_research("æœˆé¾„å®å®æ¨èå¥¶ç²‰")

if __name__ == "__main__":
    asyncio.run(main())
