import asyncio
import os
from datetime import datetime
import random
import re
from pathlib import Path
import json
import traceback

import httpx
from playwright.async_api import Page, TimeoutError as PlaywrightTimeoutError

from config.settings import (
    DEEP_RESEARCH_ENABLED,
    DEEP_RESEARCH_POST_LIMIT,
    DEEP_RESEARCH_LLM_MODEL,
    DEEP_RESEARCH_COMMENT_LIMIT,
    DEEP_RESEARCH_OUTPUT_DIR,
    SEARCH_KEYWORDS,
    BASE_URL,
    SELECTORS,
    ASR_SERVER_URL
)
from core.browser_manager import BrowserManager
from core.llm_client import LLMClient
from core.human_motion import HumanMotion
from core.video_downloader import VideoDownloader

class ResearchAgent:
    def __init__(self, browser_manager: BrowserManager, llm_client: LLMClient, recorder):
        self.browser_manager = browser_manager
        self.llm_client = llm_client
        self.page = browser_manager.page
        self.recorder = recorder  # ç»Ÿä¸€ä½¿ç”¨ recorder æ—¥å¿—ç³»ç»Ÿ
        self.human = HumanMotion(self.page)

        if not DEEP_RESEARCH_ENABLED:
            self.recorder.log("info", "æ·±åº¦ç ”ç©¶æ¨¡å¼æœªå¯ç”¨")
            return

        self.output_dir = DEEP_RESEARCH_OUTPUT_DIR / datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.recorder.log("info", f"ğŸ“‚ [æ·±åº¦ç ”ç©¶] è¾“å‡ºç›®å½•: {self.output_dir}")

        self.video_downloader = VideoDownloader(save_dir=self.output_dir / "videos")
        self.visited_note_ids = set()  # æ–°å¢ï¼šå·²è®¿é—®å¸–å­IDé›†åˆ

    async def run_deep_research(self, keyword: str = None):
        if not DEEP_RESEARCH_ENABLED:
            self.recorder.log("info", "Deep research mode is disabled. Skipping run.")
            return

        self.recorder.log("info", f"ğŸ“š [æ·±åº¦ç ”ç©¶] å¼€å§‹æ·±åº¦ç ”ç©¶: {keyword if keyword else 'configured keywords'}")

        search_term = keyword if keyword else random.choice(SEARCH_KEYWORDS)

        # æ‰§è¡Œæœç´¢
        await self._perform_search(search_term)

        # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æµè§ˆè¡Œä¸ºï¼šé€ä¸ªç‚¹å‡»å¸–å­
        research_data = []
        posts_processed = 0
        consecutive_no_new_posts = 0  # æ–°å¢ï¼šè¿ç»­æœªæ‰¾åˆ°æ–°å¸–å­çš„æ¬¡æ•°
        MAX_RETRY_WITHOUT_NEW_POST = 5  # æ–°å¢ï¼šæœ€å¤§é‡è¯•æ¬¡æ•°

        while posts_processed < DEEP_RESEARCH_POST_LIMIT:
            # 1. æ£€æŸ¥ç¯å¢ƒ
            if "xiaohongshu.com" not in self.page.url or "search_result" not in self.page.url:
                if not await self._recover_from_environment_drift(search_term):
                    break  # æ¢å¤å¤±è´¥ï¼Œç»“æŸç ”ç©¶
                continue  # æ¢å¤æˆåŠŸï¼Œé‡æ–°å¼€å§‹å¾ªç¯

            # 2. å¯»æ‰¾è§†å£å†…çš„å¸–å­
            notes = await self.page.locator(SELECTORS["note_card"]).all()
            if not notes:
                self.recorder.log("warning", "ğŸ“ [æ·±åº¦ç ”ç©¶] è§†å£æ— å¸–å­ï¼Œæ»šåŠ¨å¯»æ‰¾...")
                await self.human.human_scroll(500)
                await asyncio.sleep(2)
                notes = await self.page.locator(SELECTORS["note_card"]).all()
                if not notes:
                    self.recorder.log("error", "âŒ [æ·±åº¦ç ”ç©¶] æœªæ£€æµ‹åˆ°ç¬”è®°ï¼Œç»“æŸç ”ç©¶")
                    break

            # 3. å¯»æ‰¾æœªè®¿é—®çš„å¸–å­
            target_note, note_id = await self._find_unvisited_note(notes[:6])

            if not target_note:
                consecutive_no_new_posts += 1
                if consecutive_no_new_posts >= MAX_RETRY_WITHOUT_NEW_POST:
                    self.recorder.log("warning", "âš ï¸ [æ·±åº¦ç ”ç©¶] è¿ç»­å¤šæ¬¡æ— æ–°å¸–å­ï¼Œå¯èƒ½å·²æŠ“å–å®Œæ‰€æœ‰ç›¸å…³å†…å®¹")
                    break
                # å½“å‰è§†å£å…¨æ˜¯å·²æŠ“å–çš„ï¼Œæ»šåŠ¨åŠ è½½æ–°å†…å®¹
                self.recorder.log("info", "ğŸ“œ [å»é‡] å½“å‰è§†å£æ— æ–°å¸–å­ï¼Œæ»šåŠ¨åŠ è½½...")
                await self.human.human_scroll(random.randint(800, 1200))
                await asyncio.sleep(random.uniform(1.5, 2.5))
                continue

            # æ‰¾åˆ°æ–°å¸–å­ï¼Œé‡ç½®è®¡æ•°å™¨å¹¶è®°å½•è®¿é—®
            consecutive_no_new_posts = 0
            self.visited_note_ids.add(note_id)

            await target_note.scroll_into_view_if_needed()
            await asyncio.sleep(random.uniform(0.3, 0.5))

            self.recorder.log("info", f"ğŸ‘† [æ·±åº¦ç ”ç©¶] ç‚¹å‡»å¸–å­ {posts_processed + 1}/{DEEP_RESEARCH_POST_LIMIT} (ID: {note_id[:8]}...)")
            await target_note.click()

            # 4. ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½
            try:
                await self.page.wait_for_selector(SELECTORS["note_detail_mask"], timeout=5000)
            except:
                self.recorder.log("warning", "â±ï¸ [æ·±åº¦ç ”ç©¶] è¯¦æƒ…é¡µåŠ è½½è¶…æ—¶ï¼Œè·³è¿‡æ­¤å¸–")
                await self.page.keyboard.press("Escape")
                continue

            # 5. æå–å¸–å­å†…å®¹ï¼ˆä¸è°ƒç”¨ LLMï¼Œä»…æå–æ•°æ®ï¼‰
            post_data = await self._extract_content_from_page()
            if post_data and post_data.get("content"):
                research_data.append(post_data)
                posts_processed += 1
                self.recorder.log("info", f"âœ… [æ·±åº¦ç ”ç©¶] å·²æ”¶é›† {posts_processed}/{DEEP_RESEARCH_POST_LIMIT} ä¸ªå¸–å­")

            # 6. å…³é—­è¯¦æƒ…é¡µï¼Œè¿”å›æœç´¢ç»“æœé¡µï¼ˆç ”ç©¶æ¨¡å¼ï¼šå¿«é€Ÿå…³é—­ï¼‰
            await asyncio.sleep(random.uniform(0.5, 0.8))  # å‡åŠå»¶è¿Ÿ
            if await self.human.click_element(SELECTORS["btn_close"], "å…³é—­è¯¦æƒ…"):
                self.recorder.log("debug", "ä½¿ç”¨æŒ‰é’®å…³é—­è¯¦æƒ…é¡µ")
            else:
                await self.page.keyboard.press("Escape")
                self.recorder.log("debug", "ä½¿ç”¨ Escape å…³é—­è¯¦æƒ…é¡µ")

            # 7. ç­‰å¾…è¿”å›æœç´¢ç»“æœé¡µï¼ˆç ”ç©¶æ¨¡å¼ï¼šå¿«é€Ÿåˆ‡æ¢ï¼‰
            await asyncio.sleep(random.uniform(0.5, 1.0))  # å‡åŠå»¶è¿Ÿ

            # 8. å¦‚æœè¿˜éœ€è¦æ›´å¤šå¸–å­ï¼Œå¶å°”æ»šåŠ¨é¡µé¢åŠ è½½æ–°å†…å®¹
            if posts_processed < DEEP_RESEARCH_POST_LIMIT and posts_processed % 3 == 0:
                self.recorder.log("info", "ğŸ“œ [æ·±åº¦ç ”ç©¶] æ»šåŠ¨åŠ è½½æ›´å¤šå¸–å­...")
                await self.human.human_scroll(random.randint(800, 1200))
                await asyncio.sleep(random.uniform(1.0, 1.5))  # å‡åŠå»¶è¿Ÿ

        # ä¿å­˜ç ”ç©¶æ•°æ®
        if research_data:
            data_filename = self.output_dir / f"research_data_{search_term}.json"
            with open(data_filename, "w", encoding="utf-8") as f:
                serializable_data = []
                for item in research_data:
                    serializable_item = item.copy()
                    if 'video_local_path' in serializable_item and isinstance(serializable_item['video_local_path'], Path):
                        serializable_item['video_local_path'] = str(serializable_item['video_local_path'])
                    serializable_data.append(serializable_item)
                json.dump(serializable_data, f, ensure_ascii=False, indent=4)
            self.recorder.log("info", f"ğŸ’¾ [æ·±åº¦ç ”ç©¶] åŸå§‹æ•°æ®å·²ä¿å­˜: {data_filename}")

            report = await self._generate_report(research_data)
            await self._save_report(report, search_term)
        else:
            self.recorder.log("warning", "âš ï¸ [æ·±åº¦ç ”ç©¶] æœªæ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")

        self.recorder.log("info", f"ğŸ‰ [æ·±åº¦ç ”ç©¶] æ·±åº¦ç ”ç©¶å®Œæˆï¼å…±æ”¶é›† {len(research_data)} ä¸ªå¸–å­")


    async def _perform_search(self, keyword: str):
        self.recorder.log("info", f"ğŸ” [æœç´¢] å¼€å§‹æœç´¢å…³é”®è¯: '{keyword}'")

        try:
            # 1. ç¡®ä¿åœ¨å°çº¢ä¹¦é¦–é¡µ
            if "xiaohongshu.com" not in self.page.url or "/search_result" in self.page.url:
                self.recorder.log("info", "ğŸ” [æœç´¢] å¯¼èˆªåˆ°å°çº¢ä¹¦é¦–é¡µ...")
                await self.page.goto(BASE_URL)
                await asyncio.sleep(1)

            # 2. ç‚¹å‡»æœç´¢æ¡†
            await self.human.click_element(SELECTORS["search_input"], "æœç´¢æ¡†")
            await asyncio.sleep(random.uniform(0.5, 1.0))

            # 3. æ¸…ç©ºå¹¶è¾“å…¥å…³é”®è¯
            await self.page.locator(SELECTORS["search_input"]).clear()
            for char in keyword:
                await self.page.keyboard.type(char, delay=random.randint(50, 150))

            # 4. æäº¤æœç´¢
            self.recorder.log("info", f"ğŸ” [æœç´¢] æäº¤æœç´¢: '{keyword}'")
            await self.page.keyboard.press("Enter")

            # 5. ç­‰å¾…æœç´¢ç»“æœé¡µé¢åŠ è½½ï¼ˆå…³é”®ï¼ï¼‰
            await self.page.wait_for_load_state("networkidle", timeout=15000)

            # 6. é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿ç¬”è®°å¡ç‰‡æ¸²æŸ“å®Œæˆ
            await asyncio.sleep(3)

            self.recorder.log("info", f"âœ… [æœç´¢] æœç´¢å®Œæˆï¼Œå½“å‰URL: {self.page.url}")
        except Exception as e:
            self.recorder.log("error", f"âŒ [æœç´¢] æœç´¢å¤±è´¥ '{keyword}': {e}")
            raise


    async def _transcribe_video(self, video_local_path: Path) -> str:
        """Sends a local video file to the ASR server for transcription."""
        if not ASR_SERVER_URL:
            self.recorder.log("warning", "ASR_SERVER_URL is not configured. Skipping video transcription.")
            return ""

        if not video_local_path.exists():
            self.recorder.log("warning", f"Video file not found for transcription: {video_local_path}")
            return ""

        self.recorder.log("info", f"Sending {video_local_path.name} to ASR server for transcription...")
        try:
            async with httpx.AsyncClient(timeout=300.0) as client: # Increased timeout for large files
                with open(video_local_path, "rb") as f:
                    files = {'file': (video_local_path.name, f, 'audio/mpeg')} # Assumes mp3, adjust as needed
                    response = await client.post(ASR_SERVER_URL, files=files)
                response.raise_for_status() # Raise an exception for HTTP errors
                
                result = response.json()
                transcription = result.get("transcribed_text", "")
                if transcription:
                    self.recorder.log("info", f"ASR successful for {video_local_path.name}: {transcription[:50]}...")
                else:
                    self.recorder.log("warning", f"ASR returned empty transcription for {video_local_path.name}.")
                return transcription
        except httpx.RequestError as exc:
            self.recorder.log("error", f"ASR request error for {video_local_path.name}: {exc}")
        except httpx.HTTPStatusError as exc:
            self.recorder.log("error", f"ASR HTTP error for {video_local_path.name} - {exc.response.status_code}: {exc.response.text}")
        except Exception as e:
            self.recorder.log("error", f"Unexpected error during ASR for {video_local_path.name}: {e}")
        return ""

    async def _extract_content_from_page(self):
        """æå–å¸–å­å®Œæ•´å†…å®¹ï¼šæ ‡é¢˜ã€æ­£æ–‡ã€å›¾ç‰‡ã€è§†é¢‘ã€è¯„è®º"""
        detail = {
            "url": self.page.url,  # æ·»åŠ å½“å‰é¡µé¢URL
            "title": "", "content": "",
            "publish_date": "",  # æ–°å¢ï¼šå‘å¸ƒæ—¥æœŸ
            "image_urls": [], "video_url": "", "video_local_path": "", "media_type": "image",
            "comments": [],
            "ocr_results": {},  # Placeholder for OCR
            "asr_results": ""   # Placeholder for ASR
        }
        try:
            if await self.page.locator(SELECTORS["detail_title"]).count() > 0:
                detail["title"] = await self.page.locator(SELECTORS["detail_title"]).inner_text()

            if await self.page.locator(SELECTORS["detail_desc"]).count() > 0:
                detail["content"] = await self.page.locator(SELECTORS["detail_desc"]).inner_text()

            # æå–å‘å¸ƒæ—¥æœŸ
            detail["publish_date"] = await self._extract_publish_date()

            detail["image_urls"] = await self._extract_images()

            # æå–å¹¶ä¸‹è½½è§†é¢‘
            video_info = await self._extract_video()
            detail["video_url"] = video_info.get("video_url", "")
            detail["video_local_path"] = video_info.get("local_path", "")
            detail["media_type"] = "video" if detail["video_url"] else "image"

            # æ‰§è¡Œ ASR è½¬å½•ï¼ˆå¦‚æœæœ‰è§†é¢‘ï¼‰
            if detail["video_local_path"] and os.path.exists(detail["video_local_path"]):
                detail["asr_results"] = await self._transcribe_video(Path(detail["video_local_path"]))

            # OCR Placeholder
            if detail["image_urls"]:
                self.recorder.log("debug", "OCR åŠŸèƒ½å¾…å¼€å‘ï¼Œå½“å‰è·³è¿‡")
                detail["ocr_results"] = {"status": "skipped", "reason": "OCR service not integrated yet"}

            # 1. æ»šåŠ¨åŠ è½½æ›´å¤šä¸€çº§è¯„è®º (æœ€å¤š DEEP_RESEARCH_COMMENT_LIMIT)
            for _ in range(3): # Scroll a few times to get initial comments
                await self._scroll_comment_area()
                await asyncio.sleep(random.uniform(1, 2))

            # 2. å±•å¼€æ‰€æœ‰æŠ˜å çš„äºŒçº§è¯„è®º
            await self._expand_all_replies()
            await asyncio.sleep(random.uniform(1, 2))

            # 3. æå–è¯„è®º
            all_comments = await self._extract_comments()
            detail["comments"] = all_comments[:DEEP_RESEARCH_COMMENT_LIMIT] # Limit comments

            # æå–å¸–å­ID
            url_match = re.search(r'/explore/([a-f0-9]+)', self.page.url)
            note_id = url_match.group(1) if url_match else "unknown"

            media_count = len(detail["image_urls"]) if detail["media_type"] == "image" else 1
            self.recorder.log("info", 
                f"ğŸ“¸ [æŠ“å–] ID:{note_id[:8]}... | {detail['media_type']}x{media_count} | è¯„è®ºx{len(detail['comments'])}")

        except Exception as e:
            self.recorder.log("warning", f"å†…å®¹æå–å¼‚å¸¸: {e}")
        return detail

    async def _extract_images(self):
        """ä»è¯¦æƒ…é¡µDOMæå–æ‰€æœ‰å›¾ç‰‡URL"""
        try:
            return await self.page.evaluate("""
                () => {
                    const urls = new Set();
                    // åœ¨åª’ä½“å®¹å™¨ä¸­æŸ¥æ‰¾å›¾ç‰‡
                    const containers = document.querySelectorAll(
                        '.note-detail-mask .swiper-slide img, ' +
                        '.note-detail-mask .media-container img, ' +
                        '.note-detail-mask [class*="carousel"] img, ' +
                        '.note-detail-mask [class*="slider"] img'
                    );
                    containers.forEach(img => {
                        const src = img.src || img.dataset.src || img.getAttribute('data-src') || '';
                        if (src && (src.includes('xhscdn') || src.includes('xiaohongshu') || src.includes('sns-'))
                            && !src.includes('avatar') && !src.includes('emoji')) {
                            urls.add(src);
                        }
                    });
                    // å¤‡é€‰ï¼šdetail mask å†…æ‰€æœ‰å¤§å›¾
                    if (urls.size === 0) {
                        document.querySelectorAll('.note-detail-mask img').forEach(img => {
                            const src = img.src || img.dataset.src || '';
                            if (src && (src.includes('xhscdn') || src.includes('xiaohongshu'))
                                && !src.includes('avatar') && !src.includes('emoji')
                                && img.naturalWidth > 100) {
                                urls.add(src);
                            }
                        });
                    }
                    return [...urls];
                }
            """) or []
        except Exception as e:
            self.recorder.log("warning", f"å›¾ç‰‡æå–å¼‚å¸¸: {e}")
            return []

    async def _extract_video(self):
        """
        æå–å¹¶ä¸‹è½½è§†é¢‘
        ä½¿ç”¨ VideoDownloader ä»ç½‘é¡µ __INITIAL_STATE__ æå–è§†é¢‘ä¿¡æ¯å¹¶ä¸‹è½½
        è¿”å›åŒ…å« video_url å’Œ local_path çš„å­—å…¸
        """
        try:
            # æ­¥éª¤1: DOM å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘ç¬”è®°
            is_video = await self.page.evaluate("""
                () => {
                    const noteContainer = document.querySelector('#noteContainer, [data-type="video"]');
                    return noteContainer && noteContainer.getAttribute('data-type') === 'video';
                }
            """)

            if not is_video:
                return {"video_url": "", "local_path": ""}  # ä¸æ˜¯è§†é¢‘ç¬”è®°

            # æ­¥éª¤2: è·å–å½“å‰ URL
            current_url = self.page.url
            self.recorder.log("info", f"ğŸ“¹ [è§†é¢‘ä¸‹è½½] æ£€æµ‹åˆ°è§†é¢‘ç¬”è®°ï¼Œå¼€å§‹æå–...")

            # æ­¥éª¤3: æå–è§†é¢‘ä¿¡æ¯å¹¶ä¸‹è½½
            result = await self.video_downloader.extract_and_download(current_url)

            if result:
                self.recorder.log("info", f"âœ… [è§†é¢‘ä¸‹è½½] æˆåŠŸ")
                self.recorder.log("info", f"   URL: {result['video_url'][:60]}...")
                self.recorder.log("info", f"   æœ¬åœ°: {result['local_path']}")
                return {
                    "video_url": result["video_url"],
                    "local_path": result["local_path"],
                }
            else:
                self.recorder.log("warning", "âš ï¸ [è§†é¢‘ä¸‹è½½] æå–æˆ–ä¸‹è½½å¤±è´¥")
                return {"video_url": "", "local_path": ""}

        except Exception as e:
            self.recorder.log("error", f"âŒ [è§†é¢‘ä¸‹è½½] å¼‚å¸¸: {e}")
            return {"video_url": "", "local_path": ""}

    async def _extract_publish_date(self) -> str:
        """ä»è¯¦æƒ…é¡µæå–å‘å¸ƒæ—¥æœŸ

        Returns:
            å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¦‚ "æ˜¨å¤© 14:53 ç¦å»º"ï¼‰
            å¦‚æœæå–å¤±è´¥ï¼Œè¿”å› "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
        """
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨ï¼ˆå®¹é”™ï¼‰
            selectors = [
                '.bottom-container .date',
                '.notedetail-menu + .date',
                '[class*="bottom"] .date'
            ]
            for selector in selectors:
                element = self.page.locator(selector).first
                if await element.count() > 0:
                    date_text = await element.inner_text()
                    if date_text.strip():
                        return date_text.strip()
            return "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
        except Exception as e:
            self.recorder.log("warning", f"æ—¥æœŸæå–å¼‚å¸¸: {e}")
            return "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"

    def _extract_note_id_from_url(self, url: str) -> str:
        """ä» URL ä¸­æå– note ID

        Args:
            url: å¸–å­ URLï¼ˆå¦‚ https://www.xiaohongshu.com/explore/690b1814...)

        Returns:
            note IDï¼ˆå¦‚ 690b1814...ï¼‰ï¼Œæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        match = re.search(r'/explore/([a-f0-9]+)', url)
        return match.group(1) if match else ""

    async def _find_unvisited_note(self, notes):
        """ä»ç¬”è®°åˆ—è¡¨ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœªè®¿é—®çš„ç¬”è®°

        Args:
            notes: å¸–å­å…ƒç´ åˆ—è¡¨

        Returns:
            (target_note, note_id) å…ƒç»„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› (None, None)
        """
        for note in notes:
            href = await note.get_attribute('href')
            note_id = self._extract_note_id_from_url(href or "")
            if note_id and note_id not in self.visited_note_ids:
                return note, note_id
        return None, None

    async def _recover_from_environment_drift(self, search_term: str) -> bool:
        """ç¯å¢ƒåç¦»åçš„æ¢å¤é€»è¾‘

        å½“æ£€æµ‹åˆ°ä¸åœ¨ search_result é¡µé¢æ—¶ï¼Œå¯¼èˆªå›ä¸»é¡µå¹¶é‡æ–°æœç´¢

        Args:
            search_term: æœç´¢å…³é”®è¯

        Returns:
            True è¡¨ç¤ºæ¢å¤æˆåŠŸï¼ŒFalse è¡¨ç¤ºæ¢å¤å¤±è´¥
        """
        try:
            self.recorder.log("warning", f"âš ï¸ [ç¯å¢ƒåç¦»] å½“å‰URL: {self.page.url}")
            self.recorder.log("info", "ğŸ”„ [æ¢å¤] å¯¼èˆªå›ä¸»é¡µå¹¶é‡æ–°æœç´¢...")

            # å¯¼èˆªå›ä¸»é¡µ
            await self.page.goto("https://www.xiaohongshu.com/explore")
            await asyncio.sleep(2)

            # é‡æ–°æ‰§è¡Œæœç´¢
            await self._perform_search(search_term)

            self.recorder.log("info", "âœ… [æ¢å¤] ç¯å¢ƒæ¢å¤æˆåŠŸ")
            return True
        except Exception as e:
            self.recorder.log("error", f"âŒ [æ¢å¤] ç¯å¢ƒæ¢å¤å¤±è´¥: {e}")
            return False

    async def _extract_comments(self):
        """ä»è¯¦æƒ…é¡µDOMæå–å¯è§è¯„è®ºï¼ˆä¸€çº§+äºŒçº§ï¼‰"""
        try:
            return await self.page.evaluate("""
                () => {
                    const results = [];
                    // æŸ¥æ‰¾æ‰€æœ‰ä¸€çº§è¯„è®ºå®¹å™¨
                    const parentComments = document.querySelectorAll('.note-detail-mask .parent-comment');

                    parentComments.forEach(parentItem => {
                        try {
                            // æå–ä¸€çº§è¯„è®º
                            const mainComment = parentItem.querySelector('.comment-item:not(.comment-item-sub)');
                            if (!mainComment) return;

                            const userEl = mainComment.querySelector('.author-wrapper .name, a.name');
                            const user = userEl ? userEl.textContent.trim() : '';

                            const contentEl = mainComment.querySelector('.content .note-text');
                            const content = contentEl ? contentEl.textContent.trim() : '';

                            const likeEl = mainComment.querySelector('.like-wrapper .count');
                            const likesText = likeEl ? likeEl.textContent.trim() : '0';

                            // æå–äºŒçº§è¯„è®ºï¼ˆå­è¯„è®ºï¼‰
                            const sub_comments = [];
                            const replyContainer = parentItem.querySelector('.reply-container');
                            if (replyContainer) {
                                const subItems = replyContainer.querySelectorAll('.comment-item-sub');
                                subItems.forEach(sub => {
                                    const sUserEl = sub.querySelector('.author-wrapper .name, a.name');
                                    const sUser = sUserEl ? sUserEl.textContent.trim() : '';

                                    const sContentEl = sub.querySelector('.content .note-text');
                                    const sContent = sContentEl ? sContentEl.textContent.trim() : '';

                                    if (sContent) {
                                        sub_comments.push({ user: sUser, content: sContent });
                                    }
                                });
                            }

                            if (content) {
                                results.push({
                                    user,
                                    content,
                                    likes: parseInt(likesText.replace(/[^0-9]/g, '')) || 0,
                                    sub_comments
                                });
                            }
                        } catch(e) {
                            console.error('è¯„è®ºæå–é”™è¯¯:', e);
                        }
                    });
                    return results;
                }
            """) or []
        except Exception as e:
            self.recorder.log("warning", f"è¯„è®ºæå–å¼‚å¸¸: {e}")
            return []

    async def _scroll_comment_area(self):
        """æ»šåŠ¨è¯¦æƒ…é¡µå³ä¾§é¢æ¿ï¼ŒåŠ è½½æ›´å¤šè¯„è®º"""
        try:
            scrolled = await self.page.evaluate("""
                () => {
                    const containers = [
                        document.querySelector('.note-detail-mask .interaction-container'),
                        document.querySelector('.note-detail-mask .note-scroller'),
                        document.querySelector('.note-detail-mask [class*="contentContainer"]'),
                        document.querySelector('.note-detail-mask .right-container')
                    ];
                    for (const c of containers) {
                        if (c && c.scrollHeight > c.clientHeight) {
                            c.scrollBy({ top: 500, behavior: 'smooth' });
                            return true;
                        }
                    }
                    return false;
                }
            """)
            if scrolled:
                await asyncio.sleep(random.uniform(0.8, 1.5))
        except Exception:
            pass

    async def _expand_all_replies(self):
        """å±•å¼€æ‰€æœ‰æŠ˜å çš„äºŒçº§è¯„è®ºï¼ˆç‚¹å‡»"å±•å¼€Xæ¡å›å¤"æŒ‰é’®ï¼‰"""
        try:
            expanded_count = await self.page.evaluate("""
                () => {
                    const showMoreButtons = document.querySelectorAll('.note-detail-mask .show-more');
                    let count = 0;
                    showMoreButtons.forEach(btn => {
                        if (btn && btn.textContent.includes('å±•å¼€') && btn.textContent.includes('å›å¤')) {
                            btn.click();
                            count++;
                        }
                    });
                    return count;
                }
            """)
            if expanded_count > 0:
                self.recorder.log("info", f"ğŸ’¬ [è¯„è®º] å±•å¼€äº† {expanded_count} ä¸ªæŠ˜å çš„å›å¤")
                # ç­‰å¾…å±•å¼€çš„è¯„è®ºåŠ è½½
                await asyncio.sleep(random.uniform(1.0, 2.0))
        except Exception as e:
            self.recorder.log("warning", f"å±•å¼€å›å¤å¤±è´¥: {e}")

    async def _generate_report(self, research_data: list[dict]) -> str:
        # Placeholder for LLM report generation
        self.recorder.log("info", "Generating research report using LLM.")
        prompt = self._prepare_llm_prompt(research_data)
        # Assuming llm_client has a method like generate_text
        report = await self.llm_client.generate_text(prompt, model=DEEP_RESEARCH_LLM_MODEL)
        return report

    async def _save_report(self, report: str, keyword: str):
        report_filename = self.output_dir / f"research_report_{keyword}.md"
        with open(report_filename, "w", encoding="utf-8") as f:
            f.write(report)
        self.recorder.log("info", f"Research report saved to {report_filename}")

    def _prepare_llm_prompt(self, research_data: list[dict]) -> str:
        """
        æ„å»º LLM æç¤ºè¯ï¼šç”Ÿæˆæ•°æ®é©±åŠ¨çš„é—®é¢˜è§£å†³æŠ¥å‘Š
        ç›®æ ‡ï¼šå¸®åŠ©ç”¨æˆ·å¿«é€Ÿè·å–ç­”æ¡ˆï¼Œé¿å…é˜…è¯»å¤§é‡å¸–å­çš„ç„¦è™‘
        """
        # æå–å…³é”®è¯ï¼ˆç”¨æˆ·çš„é—®é¢˜ï¼‰
        keyword = research_data[0].get('url', '').split('keyword=')[-1].split('&')[0] if research_data else 'æœªçŸ¥ä¸»é¢˜'
        try:
            from urllib.parse import unquote
            keyword = unquote(keyword)
        except:
            pass

        prompt_parts = [
            f"# ğŸ”¬ æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆä»»åŠ¡\n\n",
            f"## ç ”ç©¶èƒŒæ™¯\n",
            f"ç”¨æˆ·åœ¨å°çº¢ä¹¦æœç´¢äº†ã€Œ**{keyword}**ã€ï¼Œé¢ä¸´ä¿¡æ¯è¿‡è½½çš„å›°æ‰°ï¼ˆéœ€é˜…è¯»å¤§é‡å¸–å­æ‰èƒ½è·å¾—ç­”æ¡ˆï¼‰ã€‚\n\n",
            f"## ä½ çš„è§’è‰²\n",
            f"ä½ æ˜¯ä¸€ä½**ä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆ**ï¼Œéœ€è¦ä»¥å­¦æœ¯ç ”ç©¶çš„ä¸¥è°¨æ€åº¦ï¼ŒåŸºäº {len(research_data)} ç¯‡å°çº¢ä¹¦å¸–å­åŠå…¶è¯„è®ºæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½**å¾ªè¯åŒ»å­¦çº§åˆ«**çš„æ·±åº¦åˆ†ææŠ¥å‘Šã€‚\n\n",
            f"## ç ”ç©¶æ–¹æ³•è®º\n",
            f"- **æ•°æ®æ¥æº**ï¼š{len(research_data)} ç¯‡å°çº¢ä¹¦çœŸå®ç”¨æˆ·å¸–å­ï¼ˆå«è¯„è®ºï¼‰\n",
            f"- **åˆ†ææ–¹æ³•**ï¼šå®šé‡ç»Ÿè®¡ + å®šæ€§åˆ†æ + å†…å®¹èšç±»\n",
            f"- **è¾“å‡ºæ ‡å‡†**ï¼šå®¢è§‚ã€é‡åŒ–ã€å¯éªŒè¯ã€å¯æ“ä½œ\n",
            f"- **ç ”ç©¶ç›®æ ‡**ï¼šç›´æ¥è§£å†³ç”¨æˆ·é—®é¢˜ï¼Œæ¶ˆé™¤å†³ç­–ç„¦è™‘\n\n",
            f"## æŠ¥å‘Šè¦æ±‚\n\n",
            f"### 1. æ ¸å¿ƒç»“è®ºï¼ˆå¿…é¡»é‡åŒ–ï¼‰\n",
            f"- ç”¨**å…·ä½“æ•°æ®**å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆä¾‹å¦‚ï¼š\"çº¦ 75% çš„å¸–å­æ¨èäº† XX å“ç‰Œ\"ï¼‰\n",
            f"- æ€»ç»“**ä¸»æµè§‚ç‚¹**åŠå…¶**æ”¯æ’‘ç†ç”±**\n",
            f"- åˆ—å‡º**å°‘æ•°æ´¾è§‚ç‚¹**åŠå…¶**ç‹¬ç‰¹è§†è§’**\n",
            f"- æ ‡æ³¨**æ•°æ®æ¥æº**ï¼šæ¯ä¸ªç»“è®ºå¿…é¡»å¼•ç”¨å…·ä½“å¸–å­URL\n\n",
            f"### 2. è¯¦ç»†åˆ†æ\n",
            f"æŒ‰ä»¥ä¸‹ç»´åº¦æ·±åº¦åˆ†æï¼š\n",
            f"- **æ¨èåº¦æ’å**ï¼šå“ªäº›é€‰é¡¹è¢«æåŠæœ€å¤šï¼Ÿå„å æ¯”å¤šå°‘ï¼Ÿ\n",
            f"- **å…³é”®é€‰æ‹©å› ç´ **ï¼šç”¨æˆ·æœ€çœ‹é‡å“ªäº›æ–¹é¢ï¼Ÿï¼ˆä»·æ ¼/å“è´¨/åŠŸæ•ˆ/å®‰å…¨æ€§ç­‰ï¼‰\n",
            f"- **å¸¸è§è¯¯åŒº**ï¼šç”¨æˆ·å®¹æ˜“è¸©çš„å‘æœ‰å“ªäº›ï¼Ÿ\n",
            f"- **äº‰è®®ç‚¹**ï¼šå“ªäº›æ–¹é¢å­˜åœ¨ä¸åŒæ„è§ï¼Ÿå„æ–¹è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
            f"- **å®ç”¨å»ºè®®**ï¼šåŸºäºæ•°æ®ç»™å‡ºçš„å¯æ“ä½œå»ºè®®\n\n",
            f"### 3. æ•°æ®ç»Ÿè®¡è¡¨æ ¼\n",
            f"| ç»´åº¦ | ç»Ÿè®¡ç»“æœ | å æ¯” | æ•°æ®æ¥æºï¼ˆå¸–å­æ•°é‡ï¼‰ |\n",
            f"|------|----------|------|---------------------|\n",
            f"| ç¤ºä¾‹ï¼šæ¨èå“ç‰ŒA | 15ç¯‡æåŠ | 75% | [å¸–å­1](url), [å¸–å­2](url)... |\n\n",
            f"### 4. è¯„è®ºæ´å¯Ÿ\n",
            f"ä»è¯„è®ºåŒºæç‚¼ï¼š\n",
            f"- çœŸå®ç”¨æˆ·ä½“éªŒï¼ˆæ­£é¢/è´Ÿé¢ï¼‰\n",
            f"- é«˜é¢‘æé—®çš„é—®é¢˜\n",
            f"- æœªè¢«è§£ç­”çš„ç–‘è™‘\n\n",
            f"---\n\n",
            f"## åŸå§‹æ•°æ®ï¼ˆå…± {len(research_data)} ç¯‡å¸–å­ï¼‰\n\n"
        ]

        # é™„åŠ è¯¦ç»†çš„å¸–å­æ•°æ®ä¾› LLM åˆ†æ
        for i, post in enumerate(research_data, 1):
            prompt_parts.append(f"### ğŸ“„ å¸–å­ {i}\n\n")
            prompt_parts.append(f"- **URL**: {post.get('url', 'N/A')}\n")
            prompt_parts.append(f"- **æ ‡é¢˜**: {post.get('title', '(æ— æ ‡é¢˜)')}\n")
            prompt_parts.append(f"- **ç±»å‹**: {post.get('media_type', 'image')}\n\n")

            # æ­£æ–‡å†…å®¹
            content = post.get('content', '').strip()
            if content:
                prompt_parts.append(f"**æ­£æ–‡å†…å®¹**ï¼š\n```\n{content}\n```\n\n")

            # è§†é¢‘è½¬å½•ï¼ˆå¦‚æœæœ‰ï¼‰
            asr_text = post.get('asr_results', '').strip()
            if asr_text:
                prompt_parts.append(f"**è§†é¢‘è½¬å½•å†…å®¹**ï¼š\n```\n{asr_text}\n```\n\n")

            # å›¾ç‰‡ä¿¡æ¯
            images = post.get('image_urls', [])
            if images:
                prompt_parts.append(f"**å›¾ç‰‡æ•°é‡**: {len(images)} å¼ \n\n")

            # è¯„è®ºæ•°æ®
            comments = post.get('comments', [])
            if comments:
                prompt_parts.append(f"**è¯„è®ºåŒº ({len(comments)} æ¡)**ï¼š\n")
                for idx, comment in enumerate(comments[:DEEP_RESEARCH_COMMENT_LIMIT], 1):
                    user = comment.get('user', 'åŒ¿å')
                    content = comment.get('content', '')
                    likes = comment.get('likes', 0)
                    sub_comments = comment.get('sub_comments', [])

                    prompt_parts.append(f"{idx}. **{user}**")
                    if likes > 0:
                        prompt_parts.append(f" (ğŸ‘ {likes})")
                    prompt_parts.append(f": {content}\n")

                    # äºŒçº§è¯„è®º
                    if sub_comments:
                        for sub in sub_comments[:3]:  # æœ€å¤šæ˜¾ç¤º3æ¡äºŒçº§è¯„è®º
                            sub_user = sub.get('user', 'åŒ¿å')
                            sub_content = sub.get('content', '')
                            prompt_parts.append(f"   â””â”€ **{sub_user}**: {sub_content}\n")

                prompt_parts.append("\n")
            else:
                prompt_parts.append("**è¯„è®ºåŒº**: æ— è¯„è®º\n\n")

            prompt_parts.append("---\n\n")

        # æœ€åçš„æŒ‡ä»¤
        prompt_parts.append("\n## âš ï¸ é‡è¦æé†’\n\n")
        prompt_parts.append("1. **æ‰€æœ‰ç»“è®ºå¿…é¡»æœ‰æ•°æ®æ”¯æ’‘**ï¼šä¸èƒ½å‡­ç©ºæ¨æµ‹ï¼Œå¿…é¡»åŸºäºä¸Šè¿°å¸–å­å†…å®¹\n")
        prompt_parts.append("2. **å¿…é¡»å¼•ç”¨æ¥æº**ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½è¦æ ‡æ³¨æ¥æºå¸–å­ç¼–å·ï¼ˆä¾‹å¦‚ï¼š[1][3][5]ï¼‰\n")
        prompt_parts.append("3. **é‡åŒ–è¡¨è¾¾**ï¼šç”¨ç™¾åˆ†æ¯”ã€å…·ä½“æ•°å­—æè¿°è¶‹åŠ¿ï¼ˆä¾‹å¦‚ï¼š\"20ç¯‡ä¸­æœ‰15ç¯‡æåˆ°...ï¼Œå 75%\"ï¼‰\n")
        prompt_parts.append("4. **å®¢è§‚ä¸­ç«‹**ï¼šå‘ˆç°å¤šå…ƒè§‚ç‚¹ï¼Œä¸åè¢’æŸä¸€ç«‹åœº\n")
        prompt_parts.append("5. **è§£å†³é—®é¢˜**ï¼šæœ€ç»ˆç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å¿«é€Ÿåšå‡ºå†³ç­–æˆ–è·å¾—ç­”æ¡ˆ\n")
        prompt_parts.append("6. **å­¦æœ¯ä¸¥è°¨**ï¼šä»¥ç ”ç©¶è€…çš„æ€åº¦ï¼Œè¿›è¡Œæ·±åº¦åˆ†æå’Œè®ºè¯\n\n")

        prompt_parts.append("## ğŸ“ æŠ¥å‘Šæ ¼å¼è¦æ±‚\n\n")
        prompt_parts.append("æŠ¥å‘Šå¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\n\n")
        prompt_parts.append("1. **æ‘˜è¦**ï¼š200å­—ä»¥å†…çš„æ ¸å¿ƒç»“è®º\n")
        prompt_parts.append("2. **é—®é¢˜èƒŒæ™¯**ï¼šç”¨æˆ·ä¸ºä»€ä¹ˆæœç´¢è¿™ä¸ªè¯é¢˜\n")
        prompt_parts.append("3. **æ•°æ®ç»Ÿè®¡**ï¼šé‡åŒ–åˆ†æï¼ˆè¡¨æ ¼å½¢å¼ï¼‰\n")
        prompt_parts.append("4. **è¯¦ç»†åˆ†æ**ï¼šå¤šç»´åº¦æ·±åº¦è§£è¯»\n")
        prompt_parts.append("5. **ç»“è®ºä¸å»ºè®®**ï¼šå¯æ“ä½œçš„å†³ç­–æŒ‡å—\n")
        prompt_parts.append("6. **å‚è€ƒæ–‡çŒ®**ï¼šåˆ—å‡ºæ‰€æœ‰å¼•ç”¨çš„å¸–å­ï¼ˆå­¦æœ¯è®ºæ–‡æ ¼å¼ï¼‰\n\n")

        prompt_parts.append("### å‚è€ƒæ–‡çŒ®æ ¼å¼ç¤ºä¾‹ï¼š\n")
        prompt_parts.append("```\n")
        prompt_parts.append("## å‚è€ƒæ–‡çŒ®\n\n")
        prompt_parts.append("[1] å°çº¢ä¹¦ç”¨æˆ·. å¸–å­æ ‡é¢˜. å°çº¢ä¹¦, å‘å¸ƒæ—¥æœŸ. [URL]\n")
        prompt_parts.append("[2] å°çº¢ä¹¦ç”¨æˆ·. å¸–å­æ ‡é¢˜. å°çº¢ä¹¦, å‘å¸ƒæ—¥æœŸ. [URL]\n")
        prompt_parts.append("...\n")
        prompt_parts.append("```\n\n")

        prompt_parts.append("è¯·ç°åœ¨å¼€å§‹ç”ŸæˆæŠ¥å‘Šï¼Œä½¿ç”¨ Markdown æ ¼å¼è¾“å‡ºã€‚\n")

        return "".join(prompt_parts)

    async def _transcribe_video(self, video_local_path: Path) -> str:
        """Sends a local video file to the ASR server for transcription."""
        if not ASR_SERVER_URL:
            self.recorder.log("warning", "ASR_SERVER_URL is not configured. Skipping video transcription.")
            return ""

        if not video_local_path.exists():
            self.recorder.log("warning", f"Video file not found for transcription: {video_local_path}")
            return ""

        self.recorder.log("info", f"Sending {video_local_path.name} to ASR server for transcription...")
        try:
            async with httpx.AsyncClient(timeout=300.0) as client: # Increased timeout for large files
                with open(video_local_path, "rb") as f:
                    # The ASR server expects 'file' parameter in multipart/form-data
                    # The filename part of the tuple should ideally be the original filename
                    files = {'file': (video_local_path.name, f, 'audio/mpeg')} 
                    response = await client.post(ASR_SERVER_URL, files=files)
                response.raise_for_status() # Raise an exception for HTTP errors
                
                result = response.json()
                transcription = result.get("transcribed_text", "")
                if transcription:
                    self.recorder.log("info", f"ASR successful for {video_local_path.name}: {transcription[:50]}...")
                else:
                    self.recorder.log("warning", f"ASR returned empty transcription for {video_local_path.name}.")
                return transcription
        except httpx.RequestError as exc:
            self.recorder.log("error", f"ASR request error for {video_local_path.name}: {exc}")
        except httpx.HTTPStatusError as exc:
            self.recorder.log("error", f"ASR HTTP error for {video_local_path.name} - {exc.response.status_code}: {exc.response.text}")
        except Exception as e:
            self.recorder.log("error", f"Unexpected error during ASR for {video_local_path.name}: {e}")
        return ""

# Example usage (for testing purposes)
async def main():
    # This requires a running Chrome instance with remote debugging on port 9222
    # and a valid Zhipu AI Key (or Kimi if LLMClient is adapted)
    
    # Temporarily set some settings for testing
    from unittest.mock import MagicMock
    browser_manager = MagicMock(spec=BrowserManager)
    browser_manager.page = MagicMock(spec=Page) # Mock the page object
    
    # Mock LLMClient to return dummy text
    llm_client = MagicMock(spec=LLMClient)
    llm_client.generate_text.return_value = "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šã€‚"

    research_agent = ResearchAgent(browser_manager, llm_client)
    await research_agent.run_deep_research("æœˆé¾„å®å®æ¨èå¥¶ç²‰")

if __name__ == "__main__":
    asyncio.run(main())
