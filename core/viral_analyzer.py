"""
çˆ†æ¬¾æ‹†è§£æ¨¡å— - åˆ†æé«˜è¡¨ç°å†…å®¹çš„æˆåŠŸæ¨¡å¼

åŠŸèƒ½ï¼š
1. æ‹†è§£çˆ†æ¬¾å†…å®¹çš„æ ‡é¢˜ã€ç»“æ„ã€æƒ…æ„Ÿ
2. æå–å¯å¤ç”¨çš„æˆåŠŸæ¨¡å¼
3. ç”Ÿæˆå†…å®¹åˆ›ä½œå»ºè®®
4. æ ‡ç­¾å’Œè¯é¢˜åˆ†æ
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import Counter
from core.analytics import ContentAnalytics
from core.title_optimizer import TitleOptimizer


class ViralAnalyzer:
    """çˆ†æ¬¾å†…å®¹æ‹†è§£å™¨"""

    def __init__(self, recorder, analytics=None):
        self.recorder = recorder
        self.analytics = analytics or ContentAnalytics(recorder)
        self.title_optimizer = TitleOptimizer(recorder)

        # æˆåŠŸæ¨¡å¼æ¨¡æ¿åº“
        self.pattern_templates = {
            "å·¥å…·æ¨è": {
                "ç»“æ„": "ç—›ç‚¹åœºæ™¯ â†’ å·¥å…·ä»‹ç» â†’ ä½¿ç”¨æ•ˆæœ â†’ æ€»ç»“æ¨è",
                "æ ‡é¢˜ç‰¹å¾": ["æ•°å­—å‹", "ç¥å™¨", "å¿…å¤‡", "æ•ˆç‡æå‡"],
                "å…³é”®è¯": ["ç¥å™¨", "å¿…å¤‡", "ç¥å™¨", "æ•‘æˆ‘", "ç›¸è§æ¨æ™š"],
                "æƒ…æ„ŸåŸºè°ƒ": "çƒ­æƒ…æ¨èã€è§£å†³ç—›ç‚¹"
            },
            "æ•™ç¨‹åˆ†äº«": {
                "ç»“æ„": "é—®é¢˜å¼•å…¥ â†’ æ­¥éª¤æ¼”ç¤º â†’ æ•ˆæœå±•ç¤º â†’ æ³¨æ„äº‹é¡¹",
                "æ ‡é¢˜ç‰¹å¾": ["ä¿å§†çº§", "æ‰‹æŠŠæ‰‹", "æ•™ç¨‹", "ä»å…¥é—¨åˆ°ç²¾é€š"],
                "å…³é”®è¯": ["ä¿å§†çº§", "æ‰‹æŠŠæ‰‹", "æ•™ç¨‹", "æ”»ç•¥"],
                "æƒ…æ„ŸåŸºè°ƒ": "è€å¿ƒæ•™å¯¼ã€å®ç”¨å¹²è´§"
            },
            "é¿å‘æŒ‡å—": {
                "ç»“æ„": "è¸©å‘ç»å† â†’ é—®é¢˜åˆ†æ â†’ è§£å†³æ–¹æ¡ˆ â†’ é¿å‘å»ºè®®",
                "æ ‡é¢˜ç‰¹å¾": ["é¿å‘", "åˆ«å†", "ä¸è¦", "é”™è¯¯"],
                "å…³é”®è¯": ["é¿å‘", "åˆ«å†", "ä¸è¦", "åƒä¸‡åˆ«"],
                "æƒ…æ„ŸåŸºè°ƒ": "çœŸè¯šæé†’ã€ç»éªŒåˆ†äº«"
            },
            "åˆé›†æ¨è": {
                "ç»“æ„": "éœ€æ±‚å¼•å…¥ â†’ å¤šä¸ªå·¥å…·æ¨è â†’ é€‚ç”¨åœºæ™¯ â†’ å–ç”¨å»ºè®®",
                "æ ‡é¢˜ç‰¹å¾": ["åˆé›†", "ç›˜ç‚¹", "æ¨è", "ç²¾é€‰"],
                "å…³é”®è¯": ["åˆé›†", "ç›˜ç‚¹", "æ¨è", "ç²¾é€‰", "å¿…çœ‹"],
                "æƒ…æ„ŸåŸºè°ƒ": "ä¸°å¯Œå…¨é¢ã€æŒ‰éœ€å–ç”¨"
            },
            "æµ‹è¯„å¯¹æ¯”": {
                "ç»“æ„": "å¯¹æ¯”èƒŒæ™¯ â†’ å¤šç»´åº¦å¯¹æ¯” â†’ ä¼˜ç¼ºç‚¹åˆ†æ â†’ é€‰æ‹©å»ºè®®",
                "æ ‡é¢˜ç‰¹å¾": ["å¯¹æ¯”", "æµ‹è¯„", "VS", "å“ªä¸ªå¥½"],
                "å…³é”®è¯": ["å¯¹æ¯”", "æµ‹è¯„", "VS", "å“ªä¸ªå¥½", "åŒºåˆ«"],
                "æƒ…æ„ŸåŸºè°ƒ": "å®¢è§‚åˆ†æã€çœŸå®ä½“éªŒ"
            }
        }

    def analyze_viral_content(self, draft: Dict, stats: Dict) -> Dict:
        """
        æ·±åº¦æ‹†è§£å•ä¸ªçˆ†æ¬¾å†…å®¹

        Args:
            draft: è‰ç¨¿æ•°æ®
            stats: ç»Ÿè®¡æ•°æ®

        Returns:
            æ‹†è§£ç»“æœå­—å…¸
        """
        title = draft.get("title", "")
        content = draft.get("content", "")
        tags = draft.get("tags", [])
        style = draft.get("style", "")

        # è®¡ç®—å†…å®¹è¡¨ç°è¯„åˆ†
        score = self.analytics.calculate_score(stats)

        analysis = {
            "draft_id": draft.get("created_at", ""),
            "title": title,
            "score": score,
            "stats": stats,
            "patterns": {}
        }

        # 1. æ ‡é¢˜åˆ†æ
        title_analysis = self._analyze_title(title)
        analysis["patterns"]["title"] = title_analysis

        # 2. å†…å®¹ç»“æ„åˆ†æ
        content_structure = self._analyze_content_structure(content)
        analysis["patterns"]["content_structure"] = content_structure

        # 3. æƒ…æ„Ÿåˆ†æ
        emotion_analysis = self._analyze_emotion(content)
        analysis["patterns"]["emotion"] = emotion_analysis

        # 4. æ ‡ç­¾åˆ†æ
        tag_analysis = self._analyze_tags(tags)
        analysis["patterns"]["tags"] = tag_analysis

        # 5. è§†è§‰åˆ†æï¼ˆå¦‚æœæœ‰å›¾ç‰‡æç¤ºè¯ï¼‰
        image_prompt = draft.get("image_prompt", "")
        if image_prompt:
            visual_analysis = self._analyze_visual(image_prompt)
            analysis["patterns"]["visual"] = visual_analysis

        # 6. æˆåŠŸè¦ç´ æ€»ç»“
        success_factors = self._extract_success_factors(analysis)
        analysis["success_factors"] = success_factors

        return analysis

    def _analyze_title(self, title: str) -> Dict:
        """åˆ†ææ ‡é¢˜ç‰¹å¾"""
        # æ ‡é¢˜è¯„åˆ†
        title_score = self.title_optimizer._calculate_score(title)

        # æå–ç‰¹å¾
        features = {
            "length": len(title),
            "score": title_score,
            "has_number": bool(re.search(r'\d+', title)),
            "has_emoji": bool(re.search(r'[^\w\s]', title)),
            "type": self._classify_title_type(title),
            "keywords": self._extract_title_keywords(title)
        }

        return features

    def _classify_title_type(self, title: str) -> str:
        """åˆ†ç±»æ ‡é¢˜ç±»å‹"""
        if re.search(r'\d+[ä¸ªæ¬¾é¡¹]', title):
            return "æ•°å­—å‹"
        elif re.search(r'[?ï¼Ÿ]', title):
            return "ç–‘é—®å‹"
        elif re.search(r'(VS|vs|å¯¹æ¯”|åŒºåˆ«)', title):
            return "å¯¹æ¯”å‹"
        elif re.search(r'(é¿å‘|åˆ«å†|ä¸è¦|åƒä¸‡åˆ«)', title):
            return "ç—›ç‚¹å‹"
        elif re.search(r'(ä¿å§†çº§|æ‰‹æŠŠæ‰‹|æ•™ç¨‹|æ”»ç•¥)', title):
            return "å¹²è´§å‹"
        elif re.search(r'(ç»äº†|å¤ªé¦™|ç›¸è§æ¨æ™š|çœŸé¦™)', title):
            return "æƒ…æ„Ÿå‹"
        else:
            return "æ™®é€šå‹"

    def _extract_title_keywords(self, title: str) -> List[str]:
        """æå–æ ‡é¢˜å…³é”®è¯"""
        # æƒ…æ„Ÿè¯æ±‡
        emotional_words = ["ç¥å™¨", "å¿…å¤‡", "ç»äº†", "å¤ªé¦™", "ç›¸è§æ¨æ™š", "çœŸé¦™",
                          "æ•‘å‘½", "èµ·é£", "ç¿»å€", "è½»æ¾", "æå®š", "è§£æ”¾"]

        keywords = []
        for word in emotional_words:
            if word in title:
                keywords.append(word)

        return keywords

    def _analyze_content_structure(self, content: str) -> Dict:
        """åˆ†æå†…å®¹ç»“æ„"""
        paragraphs = content.split('\n')
        non_empty_para = [p.strip() for p in paragraphs if p.strip()]

        structure = {
            "paragraph_count": len(non_empty_para),
            "total_length": len(content),
            "avg_paragraph_length": len(content) // max(len(non_empty_para), 1),
            "opening_type": self._classify_opening(non_empty_para[0] if non_empty_para else ""),
            "has_call_to_action": self._check_call_to_action(content),
            "scene_based": self._check_scene_based(content)
        }

        return structure

    def _classify_opening(self, opening: str) -> str:
        """åˆ†ç±»å¼€å¤´ç±»å‹"""
        if re.search(r'(æ·±å¤œ|åŠ ç­|èµ¶ç¨¿|é¢å¯¹å †ç§¯)', opening):
            return "åœºæ™¯åˆ‡å…¥"
        elif re.search(r'(è¢«é—®çˆ†|æœ€è¿‘|ç»ˆäº)', opening):
            return "æ—¶é—´å¼•å…¥"
        elif re.search(r'(ä»Šå¤©|åˆ†äº«|æ¨è)', opening):
            return "ç›´å…¥ä¸»é¢˜"
        elif re.search(r'(ä½ çŸ¥é“å—|æœ‰æ²¡æœ‰|æ˜¯ä¸æ˜¯)', opening):
            return "ç–‘é—®å¼•å…¥"
        else:
            return "å…¶ä»–"

    def _check_call_to_action(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¡ŒåŠ¨å¬å”¤"""
        cta_patterns = [
            r'è¯•è¯•', r'å…³æ³¨', r'ç‚¹èµ', r'æ”¶è—', r'è¯„è®º',
            r'è¯•è¯•çœ‹', r'è®°å¾—', r'åˆ«å¿˜äº†'
        ]
        return any(re.search(pattern, content) for pattern in cta_patterns)

    def _check_scene_based(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯åœºæ™¯åŒ–å†…å®¹"""
        scene_patterns = [
            r'æ·±å¤œ', r'åŠ ç­', r'èµ¶ç¨¿', r'æœˆåº•', r'æ€»ç»“',
            r'åŒäº‹', r'è€æ¿', r'ä»»åŠ¡', r'é¡¹ç›®'
        ]
        return any(re.search(pattern, content) for pattern in scene_patterns)

    def _analyze_emotion(self, content: str) -> Dict:
        """åˆ†ææƒ…æ„Ÿç‰¹å¾"""
        # ç—›ç‚¹è¯æ±‡
        pain_points = ["æŠ˜ç£¨", "å´©æºƒ", "å¤´ç§ƒ", "æŠ“ç‹‚", "ç„¦è™‘", "ç—›è‹¦", "çƒ¦"]
        # è§£å†³è¯æ±‡
        solutions = ["æ•‘å‘½", "ç»äº†", "å¤ªé¦™", "ç›¸è§æ¨æ™š", "çœŸé¦™", "å¥½ç”¨åˆ°å“­"]
        # æ•ˆæœè¯æ±‡
        effects = ["èµ·é£", "ç¿»å€", "è½»æ¾", "æå®š", "è§£æ”¾", "æå‡", "æ•ˆç‡"]

        pain_count = sum(1 for word in pain_points if word in content)
        solution_count = sum(1 for word in solutions if word in content)
        effect_count = sum(1 for word in effects if word in content)

        return {
            "pain_point_score": pain_count,
            "solution_score": solution_count,
            "effect_score": effect_count,
            "total_emotional_score": pain_count + solution_count + effect_count,
            "emotion_type": self._classify_emotion_type(pain_count, solution_count, effect_count)
        }

    def _classify_emotion_type(self, pain: int, solution: int, effect: int) -> str:
        """åˆ†ç±»æƒ…æ„Ÿç±»å‹"""
        if pain > 0 and solution > 0:
            return "ç—›ç‚¹-è§£å†³å‹"
        elif effect > 0:
            return "æ•ˆæœå¼ºè°ƒå‹"
        elif solution > 0:
            return "æ¨èå‹"
        else:
            return "å¹³å®å‹"

    def _analyze_tags(self, tags: List[str]) -> Dict:
        """åˆ†ææ ‡ç­¾ç‰¹å¾"""
        return {
            "tag_count": len(tags),
            "tags": tags,
            "has_ai_tag": any("AI" in tag for tag in tags),
            "has_tool_tag": any("å·¥å…·" in tag for tag in tags),
            "has_efficiency_tag": any("æ•ˆç‡" in tag for tag in tags)
        }

    def _analyze_visual(self, image_prompt: str) -> Dict:
        """åˆ†æè§†è§‰ç‰¹å¾"""
        visual_keywords = {
            "ç§‘æŠ€æ„Ÿ": ["tech", "modern", "digital", "futuristic"],
            "ç®€æ´": ["clean", "minimal", "simple", "clear"],
            "è“è‰²": ["blue", "cyan", "navy"],
            "å·¥å…·ç•Œé¢": ["interface", "UI", "screen", "workspace"]
        }

        found_features = []
        for feature, keywords in visual_keywords.items():
            if any(keyword.lower() in image_prompt.lower() for keyword in keywords):
                found_features.append(feature)

        return {
            "prompt_length": len(image_prompt),
            "visual_features": found_features,
            "has_style_keywords": len(found_features) > 0
        }

    def _extract_success_factors(self, analysis: Dict) -> List[str]:
        """æå–æˆåŠŸè¦ç´ """
        factors = []

        # åŸºäºè¯„åˆ†
        if analysis["score"] >= 70:
            factors.append("ğŸ“Š é«˜äº’åŠ¨ç‡å†…å®¹")

        # åŸºäºæ ‡é¢˜
        title = analysis["patterns"]["title"]
        if title["score"] >= 60:
            factors.append(f"ğŸ¯ é«˜åˆ†æ ‡é¢˜ ({title['type']})")
        if title["has_number"]:
            factors.append("ğŸ”¢ æ•°å­—åŒ–æ ‡é¢˜")
        if title["has_emoji"]:
            factors.append("ğŸ˜Š è¡¨æƒ…ç¬¦å·æ ‡é¢˜")

        # åŸºäºå†…å®¹ç»“æ„
        content = analysis["patterns"]["content_structure"]
        if content["scene_based"]:
            factors.append("ğŸ¬ åœºæ™¯åŒ–å†…å®¹")
        if content["has_call_to_action"]:
            factors.append("ğŸ“¢ åŒ…å«è¡ŒåŠ¨å¬å”¤")

        # åŸºäºæƒ…æ„Ÿ
        emotion = analysis["patterns"]["emotion"]
        if emotion["total_emotional_score"] >= 3:
            factors.append(f"ğŸ’­ æƒ…æ„ŸåŒ–å†…å®¹ ({emotion['emotion_type']})")

        # åŸºäºæ ‡ç­¾
        tags = analysis["patterns"]["tags"]
        if tags["has_ai_tag"] and tags["has_tool_tag"]:
            factors.append("ğŸ·ï¸ æ ‡ç­¾ç»„åˆå®Œæ•´")

        return factors if factors else ["ğŸ“ åŸºç¡€å†…å®¹"]

    def get_viral_patterns(self, top_n: int = 10) -> Dict:
        """
        è·å–çˆ†æ¬¾å†…å®¹çš„å…±åŒæ¨¡å¼

        Args:
            top_n: åˆ†æå‰ N ä¸ªé«˜è¡¨ç°å†…å®¹

        Returns:
            æ¨¡å¼åˆ†æç»“æœ
        """
        # è·å–é«˜è¡¨ç°å†…å®¹
        top_posts = self.analytics.get_top_performing(limit=top_n)

        if not top_posts:
            self.recorder.log("warning", "ğŸ“Š [çˆ†æ¬¾æ‹†è§£] æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æ")
            return {}

        # æ‹†è§£æ¯ä¸ªå†…å®¹
        analyses = []
        for item in top_posts:
            analysis = self.analyze_viral_content(item["draft"], item["stats"])
            analyses.append(analysis)

        # èšåˆåˆ†æç»“æœ
        aggregated = self._aggregate_patterns(analyses)

        return aggregated

    def _aggregate_patterns(self, analyses: List[Dict]) -> Dict:
        """èšåˆå¤šä¸ªå†…å®¹çš„æ¨¡å¼"""
        aggregated = {
            "total_analyzed": len(analyses),
            "title_patterns": {},
            "content_patterns": {},
            "emotion_patterns": {},
            "success_factors_frequency": {},
            "recommendations": []
        }

        # ç»Ÿè®¡æ ‡é¢˜ç±»å‹
        title_types = []
        title_scores = []
        for analysis in analyses:
            title_type = analysis["patterns"]["title"]["type"]
            title_types.append(title_type)
            title_scores.append(analysis["patterns"]["title"]["score"])

        from collections import Counter
        title_type_counter = Counter(title_types)
        aggregated["title_patterns"]["most_common_type"] = title_type_counter.most_common(1)[0][0]
        aggregated["title_patterns"]["type_distribution"] = dict(title_type_counter)
        aggregated["title_patterns"]["avg_score"] = sum(title_scores) / len(title_scores)

        # ç»Ÿè®¡å†…å®¹ç»“æ„
        scene_based_count = sum(1 for a in analyses if a["patterns"]["content_structure"]["scene_based"])
        cta_count = sum(1 for a in analyses if a["patterns"]["content_structure"]["has_call_to_action"])
        aggregated["content_patterns"]["scene_based_ratio"] = scene_based_count / len(analyses)
        aggregated["content_patterns"]["cta_ratio"] = cta_count / len(analyses)

        # ç»Ÿè®¡æƒ…æ„Ÿç±»å‹
        emotion_types = [a["patterns"]["emotion"]["emotion_type"] for a in analyses]
        emotion_counter = Counter(emotion_types)
        aggregated["emotion_patterns"]["most_common_type"] = emotion_counter.most_common(1)[0][0]
        aggregated["emotion_patterns"]["type_distribution"] = dict(emotion_counter)

        # ç»Ÿè®¡æˆåŠŸè¦ç´ 
        all_factors = []
        for analysis in analyses:
            all_factors.extend(analysis["success_factors"])

        factor_counter = Counter(all_factors)
        aggregated["success_factors_frequency"] = dict(factor_counter.most_common(5))

        # ç”Ÿæˆå»ºè®®
        aggregated["recommendations"] = self._generate_recommendations(aggregated)

        return aggregated

    def _generate_recommendations(self, aggregated: Dict) -> List[str]:
        """åŸºäºæ¨¡å¼åˆ†æç”Ÿæˆå»ºè®®"""
        recommendations = []

        # æ ‡é¢˜å»ºè®®
        most_common_title_type = aggregated["title_patterns"]["most_common_type"]
        recommendations.append(f"ğŸ“Œ æ ‡é¢˜å»ºè®®ï¼šä¼˜å…ˆä½¿ç”¨ {most_common_title_type} æ ‡é¢˜")

        # å†…å®¹å»ºè®®
        if aggregated["content_patterns"]["scene_based_ratio"] > 0.6:
            recommendations.append("ğŸ“Œ å†…å®¹å»ºè®®ï¼šä½¿ç”¨åœºæ™¯åŒ–æè¿°ï¼Œè®©è¯»è€…äº§ç”Ÿå…±é¸£")

        # æƒ…æ„Ÿå»ºè®®
        most_common_emotion = aggregated["emotion_patterns"]["most_common_type"]
        recommendations.append(f"ğŸ“Œ æƒ…æ„Ÿå»ºè®®ï¼šé‡‡ç”¨ {most_common_emotion} æƒ…æ„Ÿç­–ç•¥")

        # æˆåŠŸè¦ç´ å»ºè®®
        top_factors = list(aggregated["success_factors_frequency"].keys())[:3]
        if top_factors:
            recommendations.append(f"ğŸ“Œ å…³é”®è¦ç´ ï¼š{' Â· '.join(top_factors)}")

        return recommendations

    def get_content_template(self, style: str = "å·¥å…·æ¨è") -> Optional[Dict]:
        """
        è·å–æŒ‡å®šé£æ ¼çš„å†…å®¹æ¨¡æ¿

        Args:
            style: å†…å®¹é£æ ¼ï¼ˆå·¥å…·æ¨èã€æ•™ç¨‹åˆ†äº«ã€é¿å‘æŒ‡å—ç­‰ï¼‰

        Returns:
            å†…å®¹æ¨¡æ¿å­—å…¸
        """
        return self.pattern_templates.get(style)

    def apply_viral_patterns_to_content(self, content_data: Dict, patterns: Dict) -> Dict:
        """
        å°†çˆ†æ¬¾æ¨¡å¼åº”ç”¨åˆ°æ–°å†…å®¹

        Args:
            content_data: åŸå§‹å†…å®¹æ•°æ®
            patterns: çˆ†æ¬¾æ¨¡å¼åˆ†æç»“æœ

        Returns:
            ä¼˜åŒ–åçš„å†…å®¹æ•°æ®
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®åˆ†æç»“æœä¼˜åŒ–å†…å®¹
        # å…·ä½“å®ç°å¯ä»¥ç»“åˆ LLM ç”Ÿæˆä¼˜åŒ–å»ºè®®

        recommendations = patterns.get("recommendations", [])

        optimized = content_data.copy()
        optimized["optimization_notes"] = recommendations

        return optimized

    def save_analysis(self, analysis: Dict, filename: str = None):
        """ä¿å­˜åˆ†æç»“æœ"""
        if not filename:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"viral_analysis_{timestamp}.json"

        save_path = Path(__file__).parent.parent / "data" / filename

        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)

            self.recorder.log("info", f"ğŸ“Š [çˆ†æ¬¾æ‹†è§£] åˆ†æç»“æœå·²ä¿å­˜: {save_path}")
        except Exception as e:
            self.recorder.log("error", f"ğŸ“Š [çˆ†æ¬¾æ‹†è§£] ä¿å­˜å¤±è´¥: {e}")


# ä¾¿æ·å‡½æ•°
def get_viral_analyzer(recorder, analytics=None):
    """ä¾¿æ·çš„çˆ†æ¬¾åˆ†æå™¨è·å–å‡½æ•°"""
    return ViralAnalyzer(recorder, analytics)


if __name__ == "__main__":
    # æµ‹è¯•çˆ†æ¬¾æ‹†è§£åŠŸèƒ½
    from core.recorder import SessionRecorder

    recorder = SessionRecorder()
    analytics = ContentAnalytics(recorder)
    viral_analyzer = ViralAnalyzer(recorder, analytics)

    print("="*80)
    print("ğŸ“Š çˆ†æ¬¾æ‹†è§£æµ‹è¯•")
    print("="*80)

    # è·å–é«˜è¡¨ç°å†…å®¹
    top_posts = analytics.get_top_performing(limit=5)

    if top_posts:
        print(f"\næ‰¾åˆ° {len(top_posts)} ä¸ªé«˜è¡¨ç°å†…å®¹")

        # æ‹†è§£ç¬¬ä¸€ä¸ªå†…å®¹
        first_post = top_posts[0]
        print(f"\n{'='*80}")
        print("æ‹†è§£ç¬¬ä¸€ä¸ªçˆ†æ¬¾å†…å®¹")
        print(f"{'='*80}")

        analysis = viral_analyzer.analyze_viral_content(
            first_post["draft"],
            first_post["stats"]
        )

        print(f"\nğŸ“ æ ‡é¢˜: {analysis['title']}")
        print(f"ğŸ“Š è¯„åˆ†: {analysis['score']}/100")
        print(f"\nğŸ¯ æ ‡é¢˜ç‰¹å¾:")
        print(f"   ç±»å‹: {analysis['patterns']['title']['type']}")
        print(f"   è¯„åˆ†: {analysis['patterns']['title']['score']}/100")
        print(f"   é•¿åº¦: {analysis['patterns']['title']['length']} å­—ç¬¦")

        print(f"\nğŸ“„ å†…å®¹ç»“æ„:")
        print(f"   æ®µè½æ•°: {analysis['patterns']['content_structure']['paragraph_count']}")
        print(f"   åœºæ™¯åŒ–: {'æ˜¯' if analysis['patterns']['content_structure']['scene_based'] else 'å¦'}")

        print(f"\nğŸ’­ æƒ…æ„Ÿåˆ†æ:")
        print(f"   ç±»å‹: {analysis['patterns']['emotion']['emotion_type']}")
        print(f"   å¾—åˆ†: {analysis['patterns']['emotion']['total_emotional_score']}")

        print(f"\nğŸ† æˆåŠŸè¦ç´ :")
        for factor in analysis['success_factors']:
            print(f"   {factor}")

    else:
        print("\næ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æ")

    # è·å–çˆ†æ¬¾æ¨¡å¼
    print(f"\n{'='*80}")
    print("çˆ†æ¬¾æ¨¡å¼åˆ†æ")
    print(f"{'='*80}")

    patterns = viral_analyzer.get_viral_patterns(top_n=10)

    if patterns:
        print(f"\nåˆ†æå†…å®¹æ•°: {patterns['total_analyzed']}")

        print(f"\nğŸ“Š æ ‡é¢˜æ¨¡å¼:")
        print(f"   æœ€å¸¸è§ç±»å‹: {patterns['title_patterns']['most_common_type']}")
        print(f"   å¹³å‡è¯„åˆ†: {patterns['title_patterns']['avg_score']:.1f}/100")

        print(f"\nğŸ“„ å†…å®¹æ¨¡å¼:")
        print(f"   åœºæ™¯åŒ–æ¯”ä¾‹: {patterns['content_patterns']['scene_based_ratio']:.1%}")

        print(f"\nğŸ’­ æƒ…æ„Ÿæ¨¡å¼:")
        print(f"   æœ€å¸¸è§ç±»å‹: {patterns['emotion_patterns']['most_common_type']}")

        print(f"\nğŸ† æˆåŠŸè¦ç´ é¢‘ç‡:")
        for factor, count in patterns['success_factors_frequency'].items():
            print(f"   {factor}: {count} æ¬¡")

        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in patterns['recommendations']:
            print(f"   {rec}")

    print("\n" + "="*80)
