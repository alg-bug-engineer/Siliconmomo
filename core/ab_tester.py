"""
A/B æµ‹è¯•æ¡†æ¶ - æµ‹è¯•ä¸åŒå†…å®¹ç‰ˆæœ¬çš„æ•ˆæœ

åŠŸèƒ½ï¼š
1. åˆ›å»ºå’Œç®¡ç† A/B æµ‹è¯•å®éªŒ
2. è¿½è¸ªä¸åŒç‰ˆæœ¬çš„è¡¨ç°æ•°æ®
3. è‡ªåŠ¨åˆ†ææµ‹è¯•ç»“æœ
4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
5. é›†æˆåˆ°å†…å®¹åˆ›ä½œæµç¨‹
"""

import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from enum import Enum


class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€æšä¸¾"""
    PENDING = "pending"  # å¾…å¯åŠ¨
    RUNNING = "running"  # è¿è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    INCONCLUSIVE = "inconclusive"  # æ— ç»“è®º


class ABTestFramework:
    """A/B æµ‹è¯•æ¡†æ¶"""

    def __init__(self, recorder):
        self.recorder = recorder
        self.test_data_file = Path(__file__).parent.parent / "data" / "ab_tests.json"
        self._ensure_data_file()

    def _ensure_data_file(self):
        """ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨"""
        if not self.test_data_file.exists():
            with open(self.test_data_file, 'w', encoding='utf-8') as f:
                json.dump([], f)

    def create_test(
        self,
        test_name: str,
        test_type: str,
        variants: List[Dict],
        duration_days: int = 7,
        min_sample_size: int = 100
    ) -> Dict:
        """
        åˆ›å»º A/B æµ‹è¯•å®éªŒ

        Args:
            test_name: æµ‹è¯•åç§°
            test_type: æµ‹è¯•ç±»å‹ï¼ˆtitle/content/image/tagï¼‰
            variants: æµ‹è¯•å˜ä½“åˆ—è¡¨
                æ¯ä¸ª variant: {"id": "A", "content": "...", "metadata": {...}}
            duration_days: æµ‹è¯•æŒç»­å¤©æ•°
            min_sample_size: æœ€å°æ ·æœ¬é‡

        Returns:
            åˆ›å»ºçš„æµ‹è¯•å¯¹è±¡
        """
        test_id = f"test_{int(time.time())}"

        test = {
            "test_id": test_id,
            "name": test_name,
            "type": test_type,
            "status": TestStatus.PENDING.value,
            "created_at": str(time.time()),
            "duration_days": duration_days,
            "min_sample_size": min_sample_size,
            "variants": variants,
            "results": {},
            "winner": None,
            "insights": []
        }

        # åˆå§‹åŒ–æ¯ä¸ªå˜ä½“çš„ç»Ÿè®¡æ•°æ®
        for variant in variants:
            variant["stats"] = {
                "impressions": 0,
                "views": 0,
                "likes": 0,
                "collects": 0,
                "comments": 0,
                "engagement_rate": 0.0,
                "score": 0.0
            }

        self._save_test(test)
        self.recorder.log("info", f"ğŸ§ª [A/Bæµ‹è¯•] åˆ›å»ºæµ‹è¯•: {test_name} ({len(variants)} ä¸ªå˜ä½“)")

        return test

    def start_test(self, test_id: str) -> bool:
        """å¯åŠ¨æµ‹è¯•"""
        test = self._get_test(test_id)
        if not test:
            return False

        test["status"] = TestStatus.RUNNING.value
        test["started_at"] = str(time.time())
        self._update_test(test)

        self.recorder.log("info", f"ğŸ§ª [A/Bæµ‹è¯•] å¯åŠ¨æµ‹è¯•: {test['name']}")
        return True

    def record_impression(self, test_id: str, variant_id: str):
        """è®°å½•æ›å…‰"""
        test = self._get_test(test_id)
        if not test:
            return

        for variant in test["variants"]:
            if variant["id"] == variant_id:
                variant["stats"]["impressions"] += 1
                break

        self._update_test(test)

    def record_performance(
        self,
        test_id: str,
        variant_id: str,
        views: int = 0,
        likes: int = 0,
        collects: int = 0,
        comments: int = 0
    ):
        """
        è®°å½•å˜ä½“çš„è¡¨ç°æ•°æ®

        Args:
            test_id: æµ‹è¯•ID
            variant_id: å˜ä½“ID
            views: æµè§ˆé‡
            likes: ç‚¹èµæ•°
            collects: æ”¶è—æ•°
            comments: è¯„è®ºæ•°
        """
        test = self._get_test(test_id)
        if not test:
            return

        for variant in test["variants"]:
            if variant["id"] == variant_id:
                variant["stats"]["views"] += views
                variant["stats"]["likes"] += likes
                variant["stats"]["collects"] += collects
                variant["stats"]["comments"] += comments

                # è®¡ç®—äº’åŠ¨ç‡
                total_views = variant["stats"]["views"]
                if total_views > 0:
                    engagement = (likes + collects + comments) / total_views * 100
                    variant["stats"]["engagement_rate"] = round(engagement, 2)

                # è®¡ç®—ç»¼åˆè¯„åˆ†
                variant["stats"]["score"] = self._calculate_variant_score(variant["stats"])
                break

        self._update_test(test)

    def _calculate_variant_score(self, stats: Dict) -> float:
        """è®¡ç®—å˜ä½“ç»¼åˆè¯„åˆ†"""
        score = 0.0

        # äº’åŠ¨ç‡è¯„åˆ† (50åˆ†)
        engagement = stats.get("engagement_rate", 0)
        score += min(engagement * 5, 50)

        # ç»å¯¹æ•°æ®è¯„åˆ† (30åˆ†)
        views = stats.get("views", 0)
        if views >= 10000:
            score += 30
        elif views >= 5000:
            score += 25
        elif views >= 1000:
            score += 20
        elif views >= 500:
            score += 15
        elif views >= 100:
            score += 10

        # æ”¶è—ç‚¹èµæ¯” (20åˆ†)
        likes = stats.get("likes", 0)
        collects = stats.get("collects", 0)
        if likes > 0:
            ratio = collects / likes
            score += min(ratio * 10, 20)

        return round(score, 2)

    def analyze_test(self, test_id: str) -> Dict:
        """
        åˆ†ææµ‹è¯•ç»“æœ

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        test = self._get_test(test_id)
        if not test:
            return {"error": "æµ‹è¯•ä¸å­˜åœ¨"}

        if test["status"] != TestStatus.RUNNING.value:
            return {"error": "æµ‹è¯•æœªè¿è¡Œ"}

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç»“æŸæ¡ä»¶
        analysis = {
            "test_id": test_id,
            "test_name": test["name"],
            "status": test["status"],
            "can_conclude": False,
            "recommendation": None,
            "variant_comparison": [],
            "insights": []
        }

        # æ£€æŸ¥æ ·æœ¬é‡
        total_views = sum(v["stats"]["views"] for v in test["variants"])
        if total_views < test["min_sample_size"]:
            analysis["can_conclude"] = False
            analysis["recommendation"] = f"æ ·æœ¬é‡ä¸è¶³ (å½“å‰: {total_views}, éœ€è¦: {test['min_sample_size']})"
            return analysis

        # æ£€æŸ¥æµ‹è¯•æ—¶é•¿
        if "started_at" in test:
            elapsed_days = (time.time() - float(test["started_at"])) / 86400
            if elapsed_days < test["duration_days"]:
                analysis["can_conclude"] = False
                analysis["recommendation"] = f"æµ‹è¯•æ—¶é•¿ä¸è¶³ (å½“å‰: {elapsed_days:.1f}å¤©, éœ€è¦: {test['duration_days']}å¤©)"
                return analysis

        # å˜ä½“å¯¹æ¯”
        variants_with_scores = []
        for variant in test["variants"]:
            variants_with_scores.append({
                "id": variant["id"],
                "content": variant.get("content", ""),
                "score": variant["stats"]["score"],
                "views": variant["stats"]["views"],
                "engagement_rate": variant["stats"]["engagement_rate"],
                "stats": variant["stats"]
            })

        # æŒ‰è¯„åˆ†æ’åº
        variants_with_scores.sort(key=lambda x: x["score"], reverse=True)
        analysis["variant_comparison"] = variants_with_scores

        # åˆ¤æ–­æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
        if len(variants_with_scores) >= 2:
            winner = variants_with_scores[0]
            runner_up = variants_with_scores[1]
            score_diff = winner["score"] - runner_up["score"]

            if score_diff >= 10:  # è¯„åˆ†å·®å¼‚ >= 10 åˆ†
                analysis["can_conclude"] = True
                analysis["winner"] = winner["id"]
                analysis["recommendation"] = f"æ¨èä½¿ç”¨å˜ä½“ {winner['id']} (è¯„åˆ†: {winner['score']}, èƒœå‡ºä¼˜åŠ¿: {score_diff:.1f}åˆ†)"

                # ç”Ÿæˆæ´å¯Ÿ
                analysis["insights"] = self._generate_insights(test, variants_with_scores)

                # æ›´æ–°æµ‹è¯•çŠ¶æ€
                test["status"] = TestStatus.COMPLETED.value
                test["winner"] = winner["id"]
                test["results"] = analysis
                test["insights"] = analysis["insights"]
                self._update_test(test)

            else:
                analysis["can_conclude"] = False
                analysis["recommendation"] = f"å·®å¼‚ä¸æ˜¾è‘— (æœ€å¤§åˆ†å·®: {score_diff:.1f}åˆ†)ï¼Œå»ºè®®ç»§ç»­æµ‹è¯•"
                test["status"] = TestStatus.INCONCLUSIVE.value
                self._update_test(test)

        return analysis

    def _generate_insights(self, test: Dict, variants: List[Dict]) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•æ´å¯Ÿ"""
        insights = []

        test_type = test["type"]
        winner = variants[0]

        if test_type == "title":
            insights.append(f"ğŸ“Œ æ ‡é¢˜ä¼˜åŒ–ï¼š{winner['content']} è¡¨ç°æœ€ä½³")
            if winner["engagement_rate"] > 5:
                insights.append(f"âœ¨ é«˜äº’åŠ¨ç‡ ({winner['engagement_rate']:.1f}%)ï¼Œæ ‡é¢˜å¸å¼•åŠ›å¼º")

        elif test_type == "content":
            insights.append(f"ğŸ“ å†…å®¹ä¼˜åŒ–ï¼šå˜ä½“ {winner['id']} å†…å®¹ç»“æ„æ›´æœ‰æ•ˆ")
            if winner["stats"]["collects"] > winner["stats"]["likes"] * 0.5:
                insights.append(f"ğŸ’ æ”¶è—æ¯”ä¾‹é«˜ï¼Œå†…å®¹å®ç”¨æ€§å¼º")

        elif test_type == "image":
            insights.append(f"ğŸ¨ è§†è§‰ä¼˜åŒ–ï¼šå˜ä½“ {winner['id']} è§†è§‰è¡¨ç°æ›´ä½³")

        # å¯¹æ¯”åˆ†æ
        if len(variants) >= 2:
            runner_up = variants[1]
            score_diff = winner["score"] - runner_up["score"]
            lift = (score_diff / runner_up["score"] * 100) if runner_up["score"] > 0 else 0
            insights.append(f"ğŸ“Š ç›¸æ¯”ç¬¬äºŒåæå‡ {lift:.1f}%")

        return insights

    def get_winning_variant(self, test_id: str) -> Optional[Dict]:
        """è·å–è·èƒœå˜ä½“"""
        test = self._get_test(test_id)
        if not test or not test.get("winner"):
            return None

        for variant in test["variants"]:
            if variant["id"] == test["winner"]:
                return variant

        return None

    def get_all_tests(self, status: str = None) -> List[Dict]:
        """è·å–æ‰€æœ‰æµ‹è¯•"""
        try:
            with open(self.test_data_file, 'r', encoding='utf-8') as f:
                tests = json.load(f)

            if status:
                tests = [t for t in tests if t["status"] == status]

            return tests
        except:
            return []

    def _get_test(self, test_id: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šæµ‹è¯•"""
        tests = self.get_all_tests()
        for test in tests:
            if test["test_id"] == test_id:
                return test
        return None

    def _save_test(self, test: Dict):
        """ä¿å­˜æµ‹è¯•"""
        tests = self.get_all_tests()

        # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨
        existing_idx = None
        for i, t in enumerate(tests):
            if t["test_id"] == test["test_id"]:
                existing_idx = i
                break

        if existing_idx is not None:
            tests[existing_idx] = test
        else:
            tests.append(test)

        with open(self.test_data_file, 'w', encoding='utf-8') as f:
            json.dump(tests, f, indent=2, ensure_ascii=False)

    def _update_test(self, test: Dict):
        """æ›´æ–°æµ‹è¯•"""
        self._save_test(test)

    def delete_test(self, test_id: str) -> bool:
        """åˆ é™¤æµ‹è¯•"""
        tests = self.get_all_tests()
        original_count = len(tests)

        tests = [t for t in tests if t["test_id"] != test_id]

        if len(tests) < original_count:
            with open(self.test_data_file, 'w', encoding='utf-8') as f:
                json.dump(tests, f, indent=2, ensure_ascii=False)
            self.recorder.log("info", f"ğŸ§ª [A/Bæµ‹è¯•] åˆ é™¤æµ‹è¯•: {test_id}")
            return True

        return False

    def generate_summary_report(self) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦æŠ¥å‘Š"""
        tests = self.get_all_tests()

        summary = {
            "total_tests": len(tests),
            "by_status": {},
            "completed_tests": [],
            "key_insights": []
        }

        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        for status in TestStatus:
            count = sum(1 for t in tests if t["status"] == status.value)
            summary["by_status"][status.value] = count

        # å·²å®Œæˆçš„æµ‹è¯•
        completed = [t for t in tests if t["status"] == TestStatus.COMPLETED.value]
        for test in completed:
            winner = self.get_winning_variant(test["test_id"])
            summary["completed_tests"].append({
                "name": test["name"],
                "type": test["type"],
                "winner": winner["id"] if winner else None,
                "insights": test.get("insights", [])
            })

        # å…³é”®æ´å¯Ÿ
        all_insights = []
        for test in completed:
            all_insights.extend(test.get("insights", []))

        # å»é‡å¹¶ç»Ÿè®¡
        from collections import Counter
        insight_counter = Counter(all_insights)
        summary["key_insights"] = [
            {"insight": insight, "frequency": count}
            for insight, count in insight_counter.most_common(5)
        ]

        return summary


class QuickABTest:
    """å¿«é€Ÿ A/B æµ‹è¯•åŠ©æ‰‹ - ç”¨äºæ ‡é¢˜ç­‰å¿«é€Ÿæµ‹è¯•"""

    def __init__(self, recorder, ab_framework: ABTestFramework):
        self.recorder = recorder
        self.ab_framework = ab_framework

    def create_title_test(
        self,
        base_title: str,
        variants: List[str],
        duration_days: int = 3
    ) -> str:
        """
        åˆ›å»ºæ ‡é¢˜ A/B æµ‹è¯•

        Args:
            base_title: åŸºå‡†æ ‡é¢˜
            variants: å¤‡é€‰æ ‡é¢˜åˆ—è¡¨
            duration_days: æµ‹è¯•å¤©æ•°

        Returns:
            æµ‹è¯•ID
        """
        test_variants = []

        # æ·»åŠ åŸºå‡†ç‰ˆæœ¬
        test_variants.append({
            "id": "A",
            "content": base_title,
            "is_control": True
        })

        # æ·»åŠ æµ‹è¯•ç‰ˆæœ¬
        for i, variant in enumerate(variants, start=1):
            test_variants.append({
                "id": chr(65 + i),  # B, C, D, ...
                "content": variant,
                "is_control": False
            })

        test = self.ab_framework.create_test(
            test_name=f"æ ‡é¢˜æµ‹è¯• - {base_title[:20]}",
            test_type="title",
            variants=test_variants,
            duration_days=duration_days,
            min_sample_size=50  # æ ‡é¢˜æµ‹è¯•æ ·æœ¬é‡è¾ƒå°
        )

        return test["test_id"]

    def simulate_test_result(self, test_id: str):
        """æ¨¡æ‹Ÿæµ‹è¯•ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        test = self.ab_framework._get_test(test_id)
        if not test:
            return

        # ä¸ºæ¯ä¸ªå˜ä½“ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        import random
        for variant in test["variants"]:
            views = random.randint(100, 500)
            likes = random.randint(10, 100)
            collects = random.randint(5, 50)
            comments = random.randint(2, 20)

            self.ab_framework.record_performance(
                test_id,
                variant["id"],
                views=views,
                likes=likes,
                collects=collects,
                comments=comments
            )

        # åˆ†æç»“æœ
        return self.ab_framework.analyze_test(test_id)


# ä¾¿æ·å‡½æ•°
def get_ab_framework(recorder):
    """ä¾¿æ·çš„ A/B æµ‹è¯•æ¡†æ¶è·å–å‡½æ•°"""
    return ABTestFramework(recorder)


def get_quick_ab_test(recorder):
    """ä¾¿æ·çš„å¿«é€Ÿ A/B æµ‹è¯•è·å–å‡½æ•°"""
    ab_framework = ABTestFramework(recorder)
    return QuickABTest(recorder, ab_framework)


if __name__ == "__main__":
    # æµ‹è¯• A/B æµ‹è¯•æ¡†æ¶
    from core.recorder import SessionRecorder

    recorder = SessionRecorder()
    ab_framework = ABTestFramework(recorder)
    quick_test = QuickABTest(recorder, ab_framework)

    print("="*80)
    print("ğŸ§ª A/B æµ‹è¯•æ¡†æ¶æµ‹è¯•")
    print("="*80)

    # 1. åˆ›å»ºæ ‡é¢˜æµ‹è¯•
    print("\nã€åˆ›å»ºæ ‡é¢˜æµ‹è¯•ã€‘")
    base_title = "AIå·¥å…·æ¨è"
    variants = [
        "5ä¸ªAIå·¥å…·ç¥å™¨ï¼Œæ‰“å·¥äººå¿…çœ‹ï¼ğŸš€",
        "ä¸ºä»€ä¹ˆä½ çš„æ•ˆç‡è¿™ä¹ˆä½ï¼Ÿè¯•è¯•è¿™5ä¸ªAIå·¥å…·",
        "ç›¸è§æ¨æ™šï¼è¿™5ä¸ªAIå·¥å…·å¤ªé¦™äº†ï¼"
    ]

    test_id = quick_test.create_title_test(base_title, variants, duration_days=3)
    print(f"æµ‹è¯•ID: {test_id}")

    # 2. å¯åŠ¨æµ‹è¯•
    print("\nã€å¯åŠ¨æµ‹è¯•ã€‘")
    ab_framework.start_test(test_id)

    # 3. æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    print("\nã€æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®ã€‘")
    result = quick_test.simulate_test_result(test_id)

    if result:
        print(f"\nå¯ä»¥å¾—å‡ºç»“è®º: {result['can_conclude']}")
        print(f"å»ºè®®: {result['recommendation']}")

        print(f"\nã€å˜ä½“å¯¹æ¯”ã€‘")
        for variant in result['variant_comparison']:
            print(f"å˜ä½“ {variant['id']}: è¯„åˆ† {variant['score']}, æµè§ˆ {variant['views']}, äº’åŠ¨ç‡ {variant['engagement_rate']}%")

        if result.get('insights'):
            print(f"\nã€æµ‹è¯•æ´å¯Ÿã€‘")
            for insight in result['insights']:
                print(f"  {insight}")

    # 4. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ã€æ‘˜è¦æŠ¥å‘Šã€‘")
    summary = ab_framework.generate_summary_report()
    print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"æŒ‰çŠ¶æ€ç»Ÿè®¡: {summary['by_status']}")

    print("\n" + "="*80)
