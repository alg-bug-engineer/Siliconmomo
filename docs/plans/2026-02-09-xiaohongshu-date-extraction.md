# Xiaohongshu Date Extraction and Deduplication Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add publish date extraction, deduplication, and environment recovery to Xiaohongshu deep research

**Architecture:** Extend ResearchAgent with date extraction method, add visited note ID tracking, implement environment drift recovery with search replay

**Tech Stack:** Python 3.x, Playwright, asyncio, regex

---

## Task 1: Add Date Extraction Method

**Files:**
- Modify: `core/researcher.py:209-266`

**Step 1: Add the `_extract_publish_date()` method**

Add this method after `_extract_video()` method (around line 346):

```python
async def _extract_publish_date(self) -> str:
    """ä»è¯¦æƒ…é¡µæå–å‘å¸ƒæ—¥æœŸ

    Returns:
        å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¦‚ "æ˜¨å¤© 14:53 ç¦å»º"ï¼‰
        å¦‚æœæå–å¤±è´¥ï¼Œè¿”å› "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
    """
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨ï¼ˆå®¹é”™ï¼‰
        selectors = [
            '.bottom-container .date',
            '.notedetail-menu + .date',
            '[class*="bottom"] .date'
        ]
        for selector in selectors:
            element = self.page.locator(selector).first
            if await element.count() > 0:
                date_text = await element.inner_text()
                if date_text.strip():
                    return date_text.strip()
        return "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
    except Exception as e:
        self.recorder.log("warning", f"æ—¥æœŸæå–å¼‚å¸¸: {e}")
        return "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"
```

**Step 2: Update `_extract_content_from_page()` to add publish_date field**

Find the `detail` dictionary initialization (line 211-218) and add `publish_date`:

```python
detail = {
    "url": self.page.url,
    "title": "", "content": "",
    "publish_date": "",  # æ–°å¢ï¼šå‘å¸ƒæ—¥æœŸ
    "image_urls": [], "video_url": "", "video_local_path": "", "media_type": "image",
    "comments": [],
    "ocr_results": {},
    "asr_results": ""
}
```

**Step 3: Call `_extract_publish_date()` in content extraction**

Add after extracting title and content (around line 224):

```python
if await self.page.locator(SELECTORS["detail_desc"]).count() > 0:
    detail["content"] = await self.page.locator(SELECTORS["detail_desc"]).inner_text()

# æå–å‘å¸ƒæ—¥æœŸ
detail["publish_date"] = await self._extract_publish_date()

detail["image_urls"] = await self._extract_images()
```

**Step 4: Test manually**

Since this requires browser interaction, manual testing is needed:
1. Run the research agent on a test keyword
2. Check that `research_data_{keyword}.json` contains `publish_date` field
3. Verify dates are extracted or show `[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]`

**Step 5: Commit**

```bash
git add core/researcher.py
git commit -m "feat: add publish date extraction from post detail pages

- Add _extract_publish_date() method with fallback selectors
- Integrate date extraction into _extract_content_from_page()
- Return '[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]' on extraction failure"
```

---

## Task 2: Add Deduplication Infrastructure

**Files:**
- Modify: `core/researcher.py:29-43`

**Step 1: Add visited_note_ids set in __init__**

In the `__init__` method (around line 43), add:

```python
self.output_dir = DEEP_RESEARCH_OUTPUT_DIR / datetime.now().strftime("%Y%m%d_%H%M%S")
self.output_dir.mkdir(parents=True, exist_ok=True)
self.recorder.log("info", f"ğŸ“‚ [æ·±åº¦ç ”ç©¶] è¾“å‡ºç›®å½•: {self.output_dir}")

self.video_downloader = VideoDownloader(save_dir=self.output_dir / "videos")
self.visited_note_ids = set()  # æ–°å¢ï¼šå·²è®¿é—®å¸–å­IDé›†åˆ
```

**Step 2: Add `_extract_note_id_from_url()` helper method**

Add this method after `_extract_publish_date()`:

```python
def _extract_note_id_from_url(self, url: str) -> str:
    """ä» URL ä¸­æå– note ID

    Args:
        url: å¸–å­ URLï¼ˆå¦‚ https://www.xiaohongshu.com/explore/690b1814...)

    Returns:
        note IDï¼ˆå¦‚ 690b1814...ï¼‰ï¼Œæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    match = re.search(r'/explore/([a-f0-9]+)', url)
    return match.group(1) if match else ""
```

**Step 3: Add `_find_unvisited_note()` helper method**

Add this method after `_extract_note_id_from_url()`:

```python
async def _find_unvisited_note(self, notes):
    """ä»ç¬”è®°åˆ—è¡¨ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœªè®¿é—®çš„ç¬”è®°

    Args:
        notes: å¸–å­å…ƒç´ åˆ—è¡¨

    Returns:
        (target_note, note_id) å…ƒç»„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› (None, None)
    """
    for note in notes:
        href = await note.get_attribute('href')
        note_id = self._extract_note_id_from_url(href or "")
        if note_id and note_id not in self.visited_note_ids:
            return note, note_id
    return None, None
```

**Step 4: Commit**

```bash
git add core/researcher.py
git commit -m "feat: add deduplication infrastructure

- Add visited_note_ids set to track processed posts
- Add _extract_note_id_from_url() helper
- Add _find_unvisited_note() to filter visited posts"
```

---

## Task 3: Add Environment Drift Recovery

**Files:**
- Modify: `core/researcher.py:47-139`

**Step 1: Add `_recover_from_environment_drift()` method**

Add this method after the helper methods from Task 2:

```python
async def _recover_from_environment_drift(self, search_term: str) -> bool:
    """ç¯å¢ƒåç¦»åçš„æ¢å¤é€»è¾‘

    å½“æ£€æµ‹åˆ°ä¸åœ¨ search_result é¡µé¢æ—¶ï¼Œå¯¼èˆªå›ä¸»é¡µå¹¶é‡æ–°æœç´¢

    Args:
        search_term: æœç´¢å…³é”®è¯

    Returns:
        True è¡¨ç¤ºæ¢å¤æˆåŠŸï¼ŒFalse è¡¨ç¤ºæ¢å¤å¤±è´¥
    """
    try:
        self.recorder.log("warning", f"âš ï¸ [ç¯å¢ƒåç¦»] å½“å‰URL: {self.page.url}")
        self.recorder.log("info", "ğŸ”„ [æ¢å¤] å¯¼èˆªå›ä¸»é¡µå¹¶é‡æ–°æœç´¢...")

        # å¯¼èˆªå›ä¸»é¡µ
        await self.page.goto("https://www.xiaohongshu.com/explore")
        await asyncio.sleep(2)

        # é‡æ–°æ‰§è¡Œæœç´¢
        await self._perform_search(search_term)

        self.recorder.log("info", "âœ… [æ¢å¤] ç¯å¢ƒæ¢å¤æˆåŠŸ")
        return True
    except Exception as e:
        self.recorder.log("error", f"âŒ [æ¢å¤] ç¯å¢ƒæ¢å¤å¤±è´¥: {e}")
        return False
```

**Step 2: Commit**

```bash
git add core/researcher.py
git commit -m "feat: add environment drift recovery

- Add _recover_from_environment_drift() method
- Navigate to homepage and re-execute search on drift
- Return success/failure status for caller handling"
```

---

## Task 4: Integrate Deduplication and Recovery into Main Loop

**Files:**
- Modify: `core/researcher.py:47-139`

**Step 1: Add loop state variables**

At the start of `run_deep_research()`, before the main loop (around line 62):

```python
# æ‰§è¡Œæœç´¢
await self._perform_search(search_term)

# æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æµè§ˆè¡Œä¸ºï¼šé€ä¸ªç‚¹å‡»å¸–å­
research_data = []
posts_processed = 0
consecutive_no_new_posts = 0  # æ–°å¢ï¼šè¿ç»­æœªæ‰¾åˆ°æ–°å¸–å­çš„æ¬¡æ•°
MAX_RETRY_WITHOUT_NEW_POST = 5  # æ–°å¢ï¼šæœ€å¤§é‡è¯•æ¬¡æ•°

while posts_processed < DEEP_RESEARCH_POST_LIMIT:
```

**Step 2: Replace environment check with recovery logic**

Replace lines 65-67:

```python
# OLD CODE:
# if "xiaohongshu.com" not in self.page.url or "search_result" not in self.page.url:
#     self.recorder.log("error", f"âŒ [æ·±åº¦ç ”ç©¶] ç¯å¢ƒåç¦»: {self.page.url}")
#     break

# NEW CODE:
# 1. æ£€æŸ¥ç¯å¢ƒ
if "xiaohongshu.com" not in self.page.url or "search_result" not in self.page.url:
    if not await self._recover_from_environment_drift(search_term):
        break  # æ¢å¤å¤±è´¥ï¼Œç»“æŸç ”ç©¶
    continue  # æ¢å¤æˆåŠŸï¼Œé‡æ–°å¼€å§‹å¾ªç¯
```

**Step 3: Replace random note selection with deduplication logic**

Replace lines 80-86:

```python
# OLD CODE:
# # 3. é€‰æ‹©ä¸€ä¸ªå¸–å­å¹¶ç‚¹å‡»ï¼ˆç ”ç©¶æ¨¡å¼ï¼šåŠ é€Ÿæµè§ˆï¼‰
# target_note = random.choice(notes[:6])  # ä»å‰6ä¸ªä¸­éšæœºé€‰æ‹©
# await target_note.scroll_into_view_if_needed()
# await asyncio.sleep(random.uniform(0.3, 0.5))  # å‡åŠå»¶è¿Ÿ
#
# self.recorder.log("info", f"ğŸ‘† [æ·±åº¦ç ”ç©¶] ç‚¹å‡»å¸–å­ {posts_processed + 1}/{DEEP_RESEARCH_POST_LIMIT}")
# await target_note.click()

# NEW CODE:
# 3. å¯»æ‰¾æœªè®¿é—®çš„å¸–å­
target_note, note_id = await self._find_unvisited_note(notes[:6])

if not target_note:
    consecutive_no_new_posts += 1
    if consecutive_no_new_posts >= MAX_RETRY_WITHOUT_NEW_POST:
        self.recorder.log("warning", "âš ï¸ [æ·±åº¦ç ”ç©¶] è¿ç»­å¤šæ¬¡æ— æ–°å¸–å­ï¼Œå¯èƒ½å·²æŠ“å–å®Œæ‰€æœ‰ç›¸å…³å†…å®¹")
        break
    # å½“å‰è§†å£å…¨æ˜¯å·²æŠ“å–çš„ï¼Œæ»šåŠ¨åŠ è½½æ–°å†…å®¹
    self.recorder.log("info", "ğŸ“œ [å»é‡] å½“å‰è§†å£æ— æ–°å¸–å­ï¼Œæ»šåŠ¨åŠ è½½...")
    await self.human.human_scroll(random.randint(800, 1200))
    await asyncio.sleep(random.uniform(1.5, 2.5))
    continue

# æ‰¾åˆ°æ–°å¸–å­ï¼Œé‡ç½®è®¡æ•°å™¨å¹¶è®°å½•è®¿é—®
consecutive_no_new_posts = 0
self.visited_note_ids.add(note_id)

await target_note.scroll_into_view_if_needed()
await asyncio.sleep(random.uniform(0.3, 0.5))

self.recorder.log("info", f"ğŸ‘† [æ·±åº¦ç ”ç©¶] ç‚¹å‡»å¸–å­ {posts_processed + 1}/{DEEP_RESEARCH_POST_LIMIT} (ID: {note_id[:8]}...)")
await target_note.click()
```

**Step 4: Commit**

```bash
git add core/researcher.py
git commit -m "feat: integrate deduplication and recovery into main loop

- Add consecutive_no_new_posts counter with MAX_RETRY limit
- Replace environment check with recovery mechanism
- Replace random selection with deduplication logic
- Track visited note IDs to skip duplicates"
```

---

## Task 5: Update Report Generation with Publish Dates

**Files:**
- Modify: `core/researcher.py:470-598`

**Step 1: Add publish_date to post metadata in prompt**

Find the loop in `_prepare_llm_prompt()` (around line 521-526) and update:

```python
# OLD CODE:
# for i, post in enumerate(research_data, 1):
#     prompt_parts.append(f"### ğŸ“„ å¸–å­ {i}\n\n")
#     prompt_parts.append(f"- **URL**: {post.get('url', 'N/A')}\n")
#     prompt_parts.append(f"- **æ ‡é¢˜**: {post.get('title', '(æ— æ ‡é¢˜)')}\n")
#     prompt_parts.append(f"- **ç±»å‹**: {post.get('media_type', 'image')}\n\n")

# NEW CODE:
for i, post in enumerate(research_data, 1):
    prompt_parts.append(f"### ğŸ“„ å¸–å­ {i}\n\n")
    prompt_parts.append(f"- **URL**: {post.get('url', 'N/A')}\n")
    prompt_parts.append(f"- **æ ‡é¢˜**: {post.get('title', '(æ— æ ‡é¢˜)')}\n")
    prompt_parts.append(f"- **å‘å¸ƒæ—¶é—´**: {post.get('publish_date', '[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]')}\n")  # æ–°å¢
    prompt_parts.append(f"- **ç±»å‹**: {post.get('media_type', 'image')}\n\n")
```

**Step 2: Update reference format example**

Find the reference format section (around line 588-593) and update:

```python
# OLD CODE:
# prompt_parts.append("### å‚è€ƒæ–‡çŒ®æ ¼å¼ç¤ºä¾‹ï¼š\n")
# prompt_parts.append("```\n")
# prompt_parts.append("## å‚è€ƒæ–‡çŒ®\n\n")
# prompt_parts.append("[1] å°çº¢ä¹¦ç”¨æˆ·. å¸–å­æ ‡é¢˜. å°çº¢ä¹¦, å‘å¸ƒæ—¥æœŸ. [URL]\n")
# prompt_parts.append("[2] å°çº¢ä¹¦ç”¨æˆ·. å¸–å­æ ‡é¢˜. å°çº¢ä¹¦, å‘å¸ƒæ—¥æœŸ. [URL]\n")

# NEW CODE:
prompt_parts.append("### å‚è€ƒæ–‡çŒ®æ ¼å¼ç¤ºä¾‹ï¼š\n")
prompt_parts.append("```\n")
prompt_parts.append("## å‚è€ƒæ–‡çŒ®\n\n")
prompt_parts.append("[1] å°çº¢ä¹¦ç”¨æˆ·. å¸–å­æ ‡é¢˜. å°çº¢ä¹¦, æ˜¨å¤© 14:53 ç¦å»º. [URL]\n")
prompt_parts.append("[2] å°çº¢ä¹¦ç”¨æˆ·. å¸–å­æ ‡é¢˜. å°çº¢ä¹¦, 2026-02-08 10:20 åŒ—äº¬. [URL]\n")
prompt_parts.append("...\n")
```

**Step 3: Commit**

```bash
git add core/researcher.py
git commit -m "feat: integrate publish dates into report generation

- Add publish_date to post metadata in LLM prompt
- Update reference format examples to show date formats
- LLM will now use actual publish dates in citations"
```

---

## Task 6: Manual Integration Testing

**Files:**
- N/A (testing only)

**Step 1: Run full research workflow**

```bash
python run_research.py
```

**Step 2: Verify results**

Check the output in `data/deep_research_reports/`:

1. **Data file verification:**
   - Open `research_data_{keyword}.json`
   - Verify each post has `publish_date` field
   - Dates should be like "æ˜¨å¤© 14:53 ç¦å»º" or "[å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]"

2. **Deduplication verification:**
   - Check logs for "ğŸ“œ [å»é‡]" messages
   - Verify no duplicate note IDs in the data file

3. **Recovery verification:**
   - If environment drift occurred, check for:
     - "âš ï¸ [ç¯å¢ƒåç¦»]" warning
     - "ğŸ”„ [æ¢å¤]" recovery attempt
     - "âœ… [æ¢å¤] ç¯å¢ƒæ¢å¤æˆåŠŸ" confirmation

4. **Report verification:**
   - Open `research_report_{keyword}.md`
   - Check that references include dates (not "[å‘å¸ƒæ—¥æœŸä¸è¯¦]")
   - Format should be: `[N] å°çº¢ä¹¦ç”¨æˆ·. æ ‡é¢˜. å°çº¢ä¹¦, æ˜¨å¤© 14:53 ç¦å»º. [URL]`

**Step 3: Document test results**

Create a test summary in commit message format:

```
test: verify date extraction and deduplication features

Tested with keyword: [your test keyword]
Results:
- âœ… Publish dates extracted: X/Y posts
- âœ… Deduplication working: 0 duplicates found
- âœ… Environment recovery: [occurred/not occurred]
- âœ… Report references include dates

All features working as expected.
```

**Step 4: Final commit**

```bash
git add -A
git commit -m "test: verify date extraction and deduplication features

[paste your test summary here]"
```

---

## Post-Implementation Checklist

- [ ] All 6 tasks completed and committed
- [ ] Manual testing shows publish dates in reports
- [ ] No duplicate posts in research data
- [ ] Environment recovery works when tested
- [ ] Code follows DRY principles
- [ ] All error cases handled gracefully
- [ ] Ready for PR review

---

## Expected Final State

**Files Modified:** `core/researcher.py`

**New Methods Added:**
1. `_extract_publish_date()` - Extract dates from detail page
2. `_extract_note_id_from_url()` - Parse note ID from URL
3. `_find_unvisited_note()` - Find first unvisited post
4. `_recover_from_environment_drift()` - Recovery logic

**New Instance Variables:**
1. `self.visited_note_ids` - Set of processed note IDs

**Modified Methods:**
1. `__init__()` - Initialize visited_note_ids
2. `run_deep_research()` - Integrate dedup and recovery
3. `_extract_content_from_page()` - Add publish_date extraction
4. `_prepare_llm_prompt()` - Include dates in report

**Report Output:**
- References now show actual publish dates
- Format: `[N] å°çº¢ä¹¦ç”¨æˆ·. æ ‡é¢˜. å°çº¢ä¹¦, æ˜¨å¤© 14:53 ç¦å»º. [URL]`
- Fallback: `[N] å°çº¢ä¹¦ç”¨æˆ·. æ ‡é¢˜. å°çº¢ä¹¦, [å‘å¸ƒæ—¥æœŸæŠ“å–å¤±è´¥]. [URL]`
